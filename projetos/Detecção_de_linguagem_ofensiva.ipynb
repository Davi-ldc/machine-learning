{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8NNNOX4XEIP"
      },
      "source": [
        "Esse notebook tem como objetivo a identificação de discurso de odio com a ultilização do chatgpt + Alice para gerar o dataset e BERT com LSTM para a previsão."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo final esta disponivel em https://huggingface.co/spaces/DaviLima/Portuguese-offensive-lenguage"
      ],
      "metadata": {
        "id": "YxMlcuaYXbTX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B_GsKqGJQNR"
      },
      "source": [
        "# Importes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR8T4VyIFYye",
        "outputId": "ec18a99f-20b0-4199-db91-706c70fa25d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting toxigen\n",
            "  Downloading toxigen-1.1.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from toxigen) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from toxigen) (1.22.4)\n",
            "Collecting transformers==4.16.2 (from toxigen)\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from toxigen) (1.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (2.27.1)\n",
            "Collecting sacremoses (from transformers==4.16.2->toxigen)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->toxigen) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->toxigen) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->toxigen) (2022.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->toxigen) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->toxigen) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->toxigen) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->toxigen) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->toxigen) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->toxigen) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->toxigen) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2->toxigen) (2023.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->toxigen) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->toxigen) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->toxigen) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->toxigen) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->toxigen) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->toxigen) (3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->toxigen) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->toxigen) (1.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->toxigen) (1.3.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=0a366ad85fc6b010f2ef851a7ae2aa2d6b50fd31680ad210bdc5c206ba5f99cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transformers, toxigen\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.29.2\n",
            "    Uninstalling transformers-4.29.2:\n",
            "      Successfully uninstalled transformers-4.29.2\n",
            "Successfully installed sacremoses-0.0.53 toxigen-1.1.0 transformers-4.16.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.24.0-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=c5e4729a841c5641d12695d9bccba9c320c659a85afe7aaaf3ebf692f1c5934c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.24.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.3\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install toxigen\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtuOLitqFdIT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaV9GtxjFqxO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "efadc5051de448e091cd76755371cc9f",
            "a220fe561e054614a819821e35ad3d3d",
            "3f49fcb4d79c4a2599573d64b2466140",
            "0a8b6d3a3b9c48a5878b58c2f3933f69",
            "5c2f22e1d46c4c16a0c25ff0d6c726b4",
            "bc83b01ddd144e38bff29ae0920cc98a",
            "effe990ecf06444ba81f873b290ddc37",
            "be5b60b77b8d45e79651b365a69499be",
            "273c382d7a88488197e0565aca12f90e",
            "5aa07aa277b14a7da279aa0e820ee83f",
            "3ff8747619a040e8ae5bcbab7b6adab4",
            "d8d1be78d2274c6d8b53c2382f953bd5",
            "f2f2a37e44194ee9ab4cad389a29e3a1",
            "672e003592a843779aa02ae363c361e0",
            "e9059b7cfe834b5c82d2a168d406f832",
            "45d9e8546b964905bdcec026f15aa2b7",
            "3f3e976f3ba04f91a61459e91d9ff5b6"
          ]
        },
        "outputId": "599c5a8e-73bd-41a3-8174-83803f8671dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efadc5051de448e091cd76755371cc9f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# caregando o dataset"
      ],
      "metadata": {
        "id": "kS01J2s1wLkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ruanchaves/hatebr\", use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "LpX6o6liffhr",
        "outputId": "1bc07b7e-1098-4fae-885f-f3fba58c90cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-36adbb01a159>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ruanchaves/hatebr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1774\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1503\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                     ) from None\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m                     )\n\u001b[1;32m   1185\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msibling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiblings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 return HubDatasetModuleFactoryWithScript(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0mhf_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m                 dataset_info = hf_api.dataset_info(\n\u001b[0m\u001b[1;32m   1161\u001b[0m                     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mdataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTip\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \"\"\"\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_hf_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m         path = (\n\u001b[1;32m   1658\u001b[0m             \u001b[0;34mf\"{self.endpoint}/api/datasets/{repo_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_build_hf_headers\u001b[0;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   4730\u001b[0m             \u001b[0;31m# Cannot do `token = token or self.token` as token can be `False`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4731\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4732\u001b[0;31m         return build_hf_headers(\n\u001b[0m\u001b[1;32m   4733\u001b[0m             \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4734\u001b[0m             \u001b[0mis_write_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_write_action\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \"\"\"\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# Get auth token to send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mtoken_to_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_to_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0m_validate_token_to_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_to_send\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_write_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_write_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0;34m\"Token is required (`token=True`), but no token found. You\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;34m\" need to provide a token or be logged in to Hugging Face with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.DataFrame(dataset[\"train\"])\n",
        "data2 = pd.DataFrame(dataset[\"test\"])\n",
        "data = pd.concat([data1, data2])\n",
        "data"
      ],
      "metadata": {
        "id": "M1qpONrye1zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(['offensiveness_levels', 'antisemitism', 'apology_for_the_dictatorship', 'fatphobia', 'homophobia', 'partyism', 'racism', 'religious_intolerance', 'sexism', 'xenophobia', 'offensive_&_non-hate_speech', 'non-offensive', 'specialist_1_hate_speech', 'specialist_2_hate_speech', 'specialist_3_hate_speech'],axis=1)"
      ],
      "metadata": {
        "id": "6ShCNrm5rDXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "0eVSNNVerShA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# frases a mão:"
      ],
      "metadata": {
        "id": "fRYXsKGJdyXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = {\n",
        "    'querido filho Fiquei sabendo que você terá seu primeiro debate fudido na próxima semana':0,\n",
        "    'por mim agente acabava logo com essa modinha de ser gay e voltava tudo ao normal':1,\n",
        "    'os gays sofreram muito preconceito ao longo da história':0,\n",
        "    'a xenofóbia deve ser evitada':0,\n",
        "    \n",
        "    'Eu já conheço bem suas ideias antifeministas e sei que a feminista que debaterá com você é extremista':0,\n",
        "    'os comunistas são pessoas como nos':0, \n",
        "    'aquele cara é um babaca':1,\n",
        "    'quem não deve não teme':0,\n",
        "    'podemos concluir que a terra é redonda':0, \n",
        "    'todo indígena é corrupto':1, \n",
        "    'hoje foi ha loja de Dona maria comprar pão quando derepente':0,\n",
        "    \"eu quero matar os indigenas\":1,\n",
        "    \"os deficientes não fazem porra nenhuma e agente ainda sim fica sustentando eles\":1,\n",
        "    \"Os doentes mentais são inuteis\":1,  \n",
        "    \"Os mulçumanos são tudo terrorista\":1,\n",
        "    \"preto é tudo preguiçoso\":1,\n",
        "    \"As mulheres que usam roupas curtas tão pedindo para serem estupradas\":1,\n",
        "    \"Os indígenas são atrasados e precisam se integrar à civilização\":1,\n",
        "    \"Os Nordestinos são fracassados e não trabalham o suficiente\":1,\n",
        "    \"se a pessoa é pobre problema dela\":1,\n",
        "    \"Os gays são uma ameaça a família\":1,\n",
        "    \"Vamos acabar com esses comunistas\":1,\n",
        "    \"todo politico é corrupto\":1,\n",
        "    \"eu não quero meu filho perto desses animais\":1,\n",
        "    \"homossexualidade é uma doença\":1,\n",
        "    \"gays são doentes\":1, \n",
        "    \"Todas os trans deveriam ser exterminados\":1,\n",
        "    \"Mulheres são inferiores aos homens\":1,\n",
        "    \"Raça branca é superior\":1,\n",
        "    \"Os esquerdistas devem ser eliminados\":1,\n",
        "    \"Negros não são humanos\":1,\n",
        "    \"Todo o judeu deve ser enviado pro campo de concentração\":1,\n",
        "    \"As mulheres não devem ter direito ao voto\":1,\n",
        "    \"Os refugiados são uma praga e devem ser expulsos do país\":1,\n",
        "    \"Todo nordestino é folgado\":1,\n",
        "    \"Os ateus devem ser punidos com a morte por negar a existência de Deus\":1,\n",
        "    'Resumindo a tortura deve ser considerada pelo estado para obter informações':1,\n",
        "    'acaba que na prática os nosso maior problema são os gays':1,\n",
        "    'os gays merecem respeito':0,\n",
        "    \"Os índios merecem ser exterminados\":1,\n",
        "    \"As mulheres que abortam devem ser punidas com a morte\":1,\n",
        "    \"OS gays são pedófilos\":1,\n",
        "    \"Todos esses pretos deviam voltar pra a África\":1,\n",
        "    \"Os homossexuais são doentes mentais e devem ser curados à força\":1,\n",
        "    \"As mulheres são muito burras pra trabalhar\":1,\n",
        "    \"todo favelado é ladrão\":1,\n",
        "    \"Os homens são superiores às mulheres\":1,\n",
        "    \"Todo gordo é nojento\":1,\n",
        "    \"Negros devem ser separados dos brancos\":1,\n",
        "    \"Todos os LGBT são doentes mentais e devem ser internados em hospitais psiquiátricos\":1,\n",
        "    \"Judeus são uma praga\":1,\n",
        "    \"Deficientes não servem pra nada\":1,\n",
        "    \"Os índios são selvagens e devem ser civilizados\":1,\n",
        "    \"As mulheres não servem pra jogar futebol\":1,\n",
        "    'O problema é que como todos os políticos são ladrões não podemos mais confiar no sistema eleitora Brasileiro':1,\n",
        "    \"Os ateus são criminosos e devem ser punidos com a morte\":1,\n",
        "    \"As mulheres são objetos sexuais e devem ser submissas aos homens\":1,\n",
        "    \"Os judeus são responsáveis por todos os males do mundo\":1,\n",
        "    \"Os negros são preguiçosos e não merecem nada\":1,\n",
        "    \"Os homens são superiores\":1,\n",
        "    \"Os índios não merecem direitos e devem ser expulsos de suas terras\":1,\n",
        "    \"Os homossexuais devem ser condenados à morte\":1,\n",
        "    \"os ateus não têm nenhum valor moral\":1, \n",
        "    \"prefiro um filho morto do que um filho gay\":1, \n",
        "    \"os mexicanos são todos traficantes\":1, \n",
        "    \"Chines é tudo comunista\":1, \n",
        "    \"Cristãos são hipócritas fudidos\":1,\n",
        "    \"os refugiados só servem pra roubar emprego\":1,\n",
        "    \"Os pobres são preguiçosas e não se esforçam para melhorar suas vidas\":1,\n",
        "    \"Os índios são preguiçosos e vivem às custas do governo\":1,\n",
        "    \"Os ciganos são ladrões\":1,\n",
        "    \"no PT só tem bandido\":1,\n",
        "    \"Os africanos são selvagens\":1,\n",
        "    \"os autistas não servem pra nada\":1,\n",
        "    \"Os ateus são moralmente falidos e não têm valores\":1,\n",
        "    \"todo cristão é fanático\":1,\n",
        "    \"Os refugiados são um fardo para a economia\":1,\n",
        "    \"Brasileiros são inferiores aos americanos\":1,\n",
        "    \"Enforquem o Xandão\":1,\n",
        "    \"As mulheres devem ficar em casa e cuidar dos filhos\":1,\n",
        "    \"Os pobres não têm direito à educação de qualidade\":1,\n",
        "    \"Os Conservadores são fanáticos\":1,\n",
        "    \"Os deficientes não ajudam em nada a sociedade\":1,\n",
        "    \"esses pretos\":1,\n",
        "    \"aquela barriguda\":1,\n",
        "    \"os nordestinos são todos analfabetos e vivem em condições precárias\":1,\n",
        "    \"mulher fresca\":1,\n",
        "    \"bicha louca\":1,\n",
        "    \"eu não gosto de gays\":1,\n",
        "    \"essas gordas\":1,\n",
        "    \"mulheres são emocionais demais para serem líderes\":1,\n",
        "    \"os negros são criminosos e violentos\":1,\n",
        "    \"os judeus são avarentos e controlam o mundo financeiro\":1,\n",
        "    \"é negra\":1,\n",
        "    \"brasileiros são burros\":1,\n",
        "    \"ela é boa em matemática para uma mulher\":1,\n",
        "    \"então ela não pode trabalhar em atendimento ao cliente\":1,\n",
        "    \"os muçulmanos são terroristas e violentos\":1,\n",
        "    \"todo pm é corrupto\"\n",
        "    \"muçulmanos costumam ser são terroristas\":1,\n",
        "    \"aquele drogado\":1,\n",
        "    \"homem fresco\":1,\n",
        "    \"bandido bom é bandido morto\":1,\n",
        "    \"considerando que ele é gay\":1,\n",
        "    \"judeus são\":1,\n",
        "    \"eles são nojentos\":1,\n",
        "    \"os asiáticos são todos gênios em matemática e ciência\":1,\n",
        "    \"mulheres não devem trabalhar fora de casa\":1,\n",
        "    \"mulheres dirigem mal\":1,\n",
        "    \"bicha fresca\":1,\n",
        "    \"tem de se dar ao respeito\":1,\n",
        "    \"mulheres que se vestem de forma provocante estão pedindo para serem assediadas\":1,\n",
        "    \"Arabes são terroristas\":1,\n",
        "    \"macaco fedido\":1,\n",
        "    \"as pessoas com deficiência são um fardo para a sociedade\":1,\n",
        "    \"mulher só pensa em dinheiro\":1,\n",
        "    \"eu não contrataria uma mulher grávida\":1,\n",
        "    \"essa gorda\":1,\n",
        "    \"aquele preto\":1,\n",
        "    \"essa drogada\":1,\n",
        "    \"os homens são naturalmente melhores em liderança do que as mulheres\":1,\n",
        "    \"aqueles drogados\":1,\n",
        "    \"portugueses são burros\":1,\n",
        "    \"esses gordos\":1,\n",
        "    \"você não pode confiar em estrangeiros\":1,\n",
        "    \"essa fudida\":1,\n",
        "    \"Ela é muito bonita para ser negra\":1,\n",
        "    \"esses drogados\":1,\n",
        "    \"homens não prestam\":1,\n",
        "    \"aquela preta\":1,\n",
        "    \"essa mulher é muito bonita para ser lésbica\":1,\n",
        "    'Todos os cariocas são ladrões':1,\n",
        "    \"esse drogado\":1,\n",
        "    \"esses gays\":1,\n",
        "    \"barrigudo\":1,\n",
        "    \"ela não pode ser tão inteligente\":1,\n",
        "    \"todo policial é corrupto\":1,\n",
        "    \"aquele barrigudo\":1,\n",
        "    \"os brasileiros são preguiçosos e gostam de viver na malandragem\":1,\n",
        "    \"aquela drogada\":1,\n",
        "    'todo emigrante é criminoso':1,\n",
        "    \"esse gay\":1,\n",
        "    \"menina pobre engravida\":1,\n",
        "    \"adolescente pobre engravida\":1,\n",
        "     \"esses refugiados fudidos tão roubando nossos empregos\":1,\n",
        "    \"Essas feministas estão tentando acabar com a masculinidade e destruir a família\":1,\n",
        "    \"Os homossexuais são uma ameaça a todos os valores morais\":1,\n",
        "    \"Os deficientes são um fardo para a sociedade e deveriam ser eliminados\":1,\n",
        "    \"Trans são doentes mentais que precisam de cura\":1,\n",
        "    \"Os refugiados são terroristas\":1,\n",
        "    \"Esses pretos são tudo folgado\":1,\n",
        "    \"O que ocorre nessa situação é que geralmente os refugiados são todos terroristas por isso devemos tratá-los igual bandidos\":1,\n",
        "    \"O fascismo, apesar de promover a perseguição a macacos, foi um sistema que funcionou\":1,\n",
        "    \"Eu odeio todos os homossexuais. Eles são doentes e devem ser exterminados.\":1,\n",
        "    \"Mulheres são inferiores aos homens em todos os aspectos. Elas só servem para cuidar da casa e ter filhos.\":1,\n",
        "    \"Nordestinos são preguiçosos e violentos\":1,\n",
        "    \"Aqueles imigrantes estão roubando nossos empregos e recursos.\":1,\n",
        "    \"Deficientes são um fardo para a sociedade e não contribuem em nada.\":1,\n",
        "    \"Os judeus controlam o mundo e são responsáveis por todos os males da sociedade.\":1,\n",
        "    \"As mulheres que usam roupas provocantes estão pedindo para serem estupradas.\":1,\n",
        "    \"Os refugiados são criminosos e terroristas em potencial.\":1,\n",
        "    \"Como os LGBT são doentes mentais é importante ter cautela com eles\":1,\n",
        "    \"Mulheres que abortam são assassinas\":1,\n",
        "    \"A raça branca é superior a todas as outras raças e deve dominar o mundo.\":1,\n",
        "    \"Eu odeio muçulmanos\":1,\n",
        "    \"É tudo culpa dos gays\":1,\n",
        "    \"É tudo culpa dos Pobres\":1,\n",
        "    \"Esses refugiados ficam nos roubando\":1,\n",
        "    'podemos concluir que Hitler estava correto ao matar os judeos':1,\n",
        "    \n",
        "    \"As mulheres não deveriam ter direito ao voto. Elas não sabem o que é melhor para o país.\":1,\n",
        "    'Essas feministas estão tentando acabar com a masculinidade e destruir a família':1,\n",
        "    'Aqueles imigrantes estão roubando nossos empregos e recursos':1,\n",
        "    'O fascismo, apesar de promover a perseguição a macacos, foi um sistema que funcionou':1,\n",
        "    'Aqueles imigrantes estão roubando nossos empregos e recursos':1,\n",
        "    'vai se fuder':1,\n",
        "    'Os venezuelanos estão estragando o nosso país':1,\n",
        "    'A cultura Africana é ruim':1,\n",
        "    'Todo branco é chato':1,\n",
        "    'Podemos perceber que grande parte dos'\n",
        "    'Homem é tudo igual':1,\n",
        "    'A cultura africana é inferior à nossa e deve ser suprimida':1,\n",
        "    'O islamismo é uma ameaça à segurança nacional e deve ser proibido':1,\n",
        "    'O budismo é uma farsa e todas as suas práticas devem ser banidas':1,\n",
        "    'Porém como os negros são mais propícios a cometerem crimes o algoritimo estava certo':1,\n",
        "    'Como os mais pobres não entendem de política não devem votar':1,\n",
        "\n",
        "    'Os pobres estão em ultimo na pirâmide social':0,\n",
        "    'O aquecimento global vem afetando o mundo nos últimos anos':0,\n",
        "    'esse fator é algo natural porem está aumentando drasticamente pela quantidade de combustíveis quem são queimados diariamente':0,\n",
        "    'assim afetando o ambiente onde vivemos':0,\n",
        "    'O comunismo se provou uma falha com a união soviética':0,\n",
        "    'ocasionando uma elevação na temperatura ambiente':0,\n",
        "    'A partir disso surgem vários problemas que afetam o habitat natural de vários animais':0,\n",
        "    'e um deles e o derretimento das geleiras que obrigam os animais como o urso polar a mudar seus hábitos de caça':0,\n",
        "    'Muitos ursos para se alimentarem começaram a invadir cidades':0,\n",
        "    'a relatos de que os ursos atacam pessoas para se alimentar':0,\n",
        "    'Muitos estão pensando na hipótese de caça aos ursos porem esse método e proibido em lei':0,\n",
        "    'a agência do Meio Ambiente russa negou a licença para abater essa espécie pois a mesma se encontra em extinção':0,\n",
        "    'medidas são necessárias para resolver esse impasse podendo ter algum órgão governamental responsável pelo meio ambiente para se responsabilizar de resgatar e alimentar esses animais para evitar futuros ataques contra pessoas':0,\n",
        "    'A Rússia o quinto pais do ranking de emissores de queima de combustíveis para que ela possa reduzir a queima de combustíveis deve investir em um combustível mais \"limpo\" para o ambiente por exemplo o etanol além de estimular o uso do mesmo':0,\n",
        "    'No âmbito escolar':0,\n",
        "    'a discriminação vem gerando cada vez mais atos de violências devido a diversidade de opções sexuais':0,\n",
        "    'A partir destes aspectos os preconceitos se originam':0,\n",
        "    'mas o respeito é direito e dever de toda a população':0,\n",
        "    'O preconceito dos alunos com os outros que optam por uma escolha sexual diferente é perceptível':0,\n",
        "    'como consequência seguinte a violência não sendo ela apenas física':0,\n",
        "    'também sentimental e etc':0,\n",
        "    'É frequente a decorrência de fatos de discriminação por esse motivo':0,\n",
        "    'como exemplo podemos citar inúmeros casos':0,\n",
        "    'que já foi retratado pela Rede Globo que homossexuais foram vítimas de preconceito e discriminação':0,\n",
        "    'Segundo a Constituição Brasileira':0,\n",
        "    'todos são iguais perante a lei e todos os brasileiros e estrangeiros residentes tem direito à vida':0,\n",
        "    'à segurança e à propriedade sem separação de qualquer natureza':0,\n",
        "    'sendo um problema á sociedade':0,\n",
        "    'as rejeições estão presentes no nosso cotidiano escolar':0,\n",
        "    'de uma criação de programas sobre o direito e dever dos alunos de respeitar as diferenças sexuais de seus colegas':0,\n",
        "    'contando com o apoio da secretaria municipal de educação para desenvolver palestras educativas':0,\n",
        "    'os pais pais também precisam estar colaborando para que os resultados sejam efetivos gerando assim uma convivência pacifica dentro e fora do ambiente escolar':0,\n",
        "    'O conhecimento é fundamental para que haja uma opinião crítica sobre um determinado assunto':0,\n",
        "    'esse é o ponto de partida de um argumento num debate':0,\n",
        "    'Sobretudo há um grande problema com grande parte da população brasileira':0,\n",
        "    'que é a falta de leitura':0,\n",
        "    'O Brasil é um país com uma baixa taxa de leitores':0,\n",
        "    'de acordo com pesquisas':0,\n",
        "    'muito dos brasileiros tem deixado os livros em segundo plano':0,\n",
        "    'isso porque o uso de celulares':0,\n",
        "    'tabletes e notebooks são os maiores concorrentes com o popular e conhecido livro':0,\n",
        "    'além do conhecimento':0,\n",
        "    'novos horizontes para que as Todos possam ter opiniões formadas e entender sobre diversos assuntos':0,\n",
        "    'Assim num debate':0,\n",
        "    'ou numa discussão':0,\n",
        "    'não haverá aquele velho discurso que todos usam e que ninguém nem sabe o que realmente significa':0,\n",
        "    'mas que compartilham em redes sociais como se fossem os próprios autores':0,\n",
        "    'Um debate que gera resultados satisfatórios é feito por pessoas com habilidades adquiridas através da leitura':0,\n",
        "    'da busca pelo conhecimento e pela criticidade':0,\n",
        "    'debater não significa brigar':0,\n",
        "    'é modo de cada um mostrar com argumentos válidos seu posicionamento':0,\n",
        "    'é uma conversa que pode gerar controvérsias':0,\n",
        "    'mas que é aceita e no final agrega sabedoria':0,\n",
        "    'Querido filho':0,\n",
        "    'Fiquei sabendo que você terá seu primeiro debate escolar na próxima semana':0,\n",
        "    'Eu já conheço bem suas ideias antifeministas e sei que a feminista que debaterá com você é extremista':0,\n",
        "    'Sendo seu pai':0,\n",
        "    'sinto que é meu dever te lhe dar alguns conselhos':0,\n",
        "    'Quando for debater':0,\n",
        "    'mantenha a calma e não ataque a argumentadora oponente':0,\n",
        "    'mas sim seus argumentos':0,\n",
        "    'Esse é o primeiro passo para que o debate seja civilizado e produtivo':0,\n",
        "    'Use a razão e a lógica':0,\n",
        "    'Caso a feminista faça o oposto disso':0,\n",
        "    'descarte os seus argumentos meramente por você ser homem':0,\n",
        "    'heterossexual e cristão':0,\n",
        "    'explique ao público que ela está usando a famosa falácia ad hominem':0,\n",
        "    'Evite e aponte':0,\n",
        "    'meu querido filho':0,\n",
        "    'todas as falácias lógicas':0,\n",
        "    'Falácias destroem o bom debate':0,\n",
        "    'Caso a feminista se esquive das críticas contra o feminismo':0,\n",
        "    'afirmando que aquilo que está sendo criticado por você chama-se \"femismo\" e não \"feminismo\"':0,\n",
        "    'avise-a que ela está utilizando a falácia do escocês de verdade':0,\n",
        "    'Mantenha-se firme também na defesa pela liberdade de expressão':0,\n",
        "    'Essa é outra regra de ouro para o bom debate':0,\n",
        "    'Não importa o quão verbalmente agressiva seja a feminista ou quão repulsivas pareçam ser suas ideias':0,\n",
        "    'lembre-se que ela possui o direito de falar tudo aquilo que realmente pensa':0,\n",
        "    'Se tudo o que ela fala for de fato tão horrível e irracional assim':0,\n",
        "    'então será fácil para você desconstruir publicamente os argumentos dela':0,\n",
        "    'Lembre-se daquilo que sempre te ensinei: tentar calar o debatedor adversário à força':0,\n",
        "    'como se estivesse cometendo um crime por expor o que pensa':0,\n",
        "    'é para aqueles que não conseguem convencer os demais através do uso da lógica e da razão':0,\n",
        "    'então precisam fazer isso através de meios coercitivos':0,\n",
        "    'se alguém nesse debate utilizar constantemente argumentos falaciosos ou tentar calar à força o adversário':0,\n",
        "    'alegando estar combatendo um \"discurso de ódio\"':0,\n",
        "    'então que esse alguém não seja você':0,\n",
        "    'Não podemos controlar a feminista com quem você debaterá':0,\n",
        "    'mas você tem controle sobre si próprio':0,\n",
        "    'Faça a sua parte e tenha um bom debate! Com carinho':0,\n",
        "    'Seu amado Pai':0,\n",
        "    'Na revolução Industrial':0,\n",
        "    'o desemprego aumentou':0,\n",
        "    'a mão de obra foi substituída por máquinas':0,\n",
        "    'causando uma desigualdade social':0,\n",
        "    'por conta da divisão de classes':0,\n",
        "    'por razões racionais':0,\n",
        "    'que estão enraizadas':0,\n",
        "    'desde a colonização do Brasil':0,\n",
        "    'que já ocorria por uma hierarquia':0,\n",
        "    'que hordienamente não é diferente':0,\n",
        "    'No livro memórias póstumas de Brás Cubas de Machado de Assis':0,\n",
        "    'que o garoto chamado de Brás Cubas tinha coo brinquedo':0,\n",
        "    'um negrinho de estimação':0,\n",
        "    'que lhe servia de montaria':0,\n",
        "    'podemos perceber que havia uma superioridade pela parte de Brás Cubas':0,\n",
        "    'uma desigualdade social':0,\n",
        "    'os negros eram considerados como objetos e isto ocorreu como consquências como preconceito racial':0,\n",
        "    'Em consequência disso':0,\n",
        "    'famílias de baixas rendas':0,\n",
        "    'principalmente familias negras':0,\n",
        "    'que muitas crianças e jovens não possuem oportunidade de estudo':0,\n",
        "    'pois muitas vezzes tem que troar estudar para trabalharem como forma de sobreviver':0,\n",
        "    'Desta forma ocorre dificuldade de obter uma igualdade social':0,\n",
        "    'pois segundo Paulo Freire':0,\n",
        "    'se a educação sozinha não transforma a sociedade':0,\n",
        "    'a sociedade muda':0,\n",
        "    'para termos mais igualdade':0,\n",
        "    'a educação precisa ser mais preservada sem ser limitada por condição econômica':0,\n",
        "    'todos somos iguais e merecemos direitos iguais':0,\n",
        "    'o ministerio de educação poderia investir mais na educação de crianças e jovens em bairros de baixa renda':0,\n",
        "    'para obterem um futuro melhor e um Brasil mais igualitário':0,\n",
        "    'para diminuir divisão de classes':0,\n",
        "    'poderia investir nos cidadãos para serem empreendedores ou seja terem seu proprio negócio':0,\n",
        "    'obtendo cursos gratuitos como beleza':0,\n",
        "    'culinária entre outras áreas':0,\n",
        "    'pessoas mais independentes':0,\n",
        "     'Desde dos primòrdio da história':0,\n",
        "    'observam o trabalho infantil':0,\n",
        "    'Com o passar dos anos houve uma diminuição':0,\n",
        "    'porém não conseguiram acabar com este mau':0,\n",
        "    'Criança não tem que trabalhar':0,\n",
        "    'mas sim estudar':0,\n",
        "    'faz parte da formação':0,\n",
        "    'Os direitos humanos defendem as crianças desta exploração':0,\n",
        "    'não é eficaz o suficiente':0,\n",
        "    'Assegurar o direito e dignidade de uma criança è um dever da sociedade':0,\n",
        "    'brincar è necéssario para a formação de futuros adultos':0,\n",
        "    'A educação de hoje serà os resultados de amanhã':0,\n",
        "    'ainda muitas crianças trocam os livros e brinquedos':0,\n",
        "    'por enxadas e serviços duros':0,\n",
        "    '5 milhões de crianças não estudam':0,\n",
        "    'Segundo os direitos humanos':0,\n",
        "    'A criança será protegida cõntra qualquer crueldade e exploração':0,\n",
        "    'Não será permitido que trabalhem ou tenha ocupação que prejudique os estudos e a saúde':0,\n",
        "    'É possível afirmar que 5':0,\n",
        "    '438 milhões de crianças e adolescentes de 5 à 17 anos trabalhem no país':0,\n",
        "    'grande parte fica no  nordeste 42':0,\n",
        "    'Este problema tem culpa da família e sociedade':0,\n",
        "    'quando simplismente ignoram':0,\n",
        "    'em decorrência da postura individualista':0,\n",
        "    'levando em consideração o capitalismo comteporâneo':0,\n",
        "    'sem pratica e conteudo ético':0,\n",
        "    'Crianças não devem trabalhar':0,\n",
        "    'portanto que as crianças devem estudar e ser protegidas contra exploração':0,\n",
        "    'Para solucionar o caso':0,\n",
        "    'será necéssario a ajuda do governo juntamente  com o poder executivo':0,\n",
        "    'para que as leis existente fiquem mais eficaz':0,\n",
        "    'Jà o individuo e sociedade deverar agir de forma nacional':0,\n",
        "    'não permitindo que o problema aconteça':0,\n",
        "    'podem fazer denuncias':0,\n",
        "    'assim a policia e justica poderá identificar a pessoa que comete tal crime':0,\n",
        "    'Dessa forma podem obter resultados positivos':0,\n",
        "    'podemos concluir que os games estarão cada vez mais presentes na educação':0,\n",
        "    'eu adoro sorvete. ai hoje quando eu tava na frente da padaria eu fiquei puto com preço':0,\n",
        "    \"meu Pc novo é muito foda\":0,\n",
        "    \"andava aos poucos em busca de alguma pista\":0,\n",
        "    \"Eu gosto de passar tempo com minha família e amigos.\":0,\n",
        "    \"A igualdade de direitos é importante para a construção de uma sociedade justa.\":0,\n",
        "    \"A liberdade de expressão é um direito fundamental de todos os indivíduos.\":0,\n",
        "    \"Eu amo viajar e conhecer novas culturas.\":0,\n",
        "    \"Muitos Nordestinos sofrem com a seca\":0,\n",
        "    \"Todas merecem respeito independentemente da sua orientação sexual.\":0,\n",
        "    \"Merecemos ter direito à educação e oportunidades de trabalho.\":0,\n",
        "    \"O amor e o respeito devem ser dados a todas as religiões.\":0,\n",
        "    \"Eu valorizo a diversidade e a inclusão em todos os aspectos da vida.\":0,\n",
        "    \"Todos merecem igualdade de acesso aos recursos básicos, como água e comida.\":0,\n",
        "    \"Acredito que a empatia e a compaixão são fundamentais para um mundo melhor.\":0,\n",
        "    \"Eu amo ler livros e aprender coisas novas.\":0,\n",
        "    \"Acredito que a empatia e a compaixão são essenciais para uma sociedade justa e igualitária.\":0,\n",
        "    \"Eu acredito que a educação é a chave para um futuro melhor para todos.\":0,\n",
        "    \"Devemos ser julgados pelo nosso caráter, não pela aparência ou origem.\":0,\n",
        "    \"Acredito que é importante proteger o meio ambiente para as gerações futuras.\":0,\n",
        "    \"Eu acredito que todos os seres humanos têm o direito de serem tratados com respeito e dignidade.\":0,\n",
        "    \"Com o decorrer do tempo, a globalização se tornou cada vez mais frequente no cotidiano da humanidade, havendo uma influência bastante significativa na vida das pessoas\":0,\n",
        "    \"A globalização trouxe consigo muitos fatores positivos, taus como a integração econômica, social, cultural e política facilitando a interação com outros países pelo mundo\":0,\n",
        "    \"Porém, causou um grande descontentamento para muitos, pois impulsionou ao aumento da desigualdade social, e justamente por ser uma proposta que iria beneficiar a todos, acabou almejando a economia do país esquecendo da situação dos cidadãos que nele habitam\":0,\n",
        "    \"É de extrema importância um país se globalizar, contudo deve analisar a situação de determinados habitantes antes de efetivar negócios com outro país\":0,\n",
        "    \"Não seria possível combater esse descaso de uma única vez, porém com a contribuição de órgão públicos poderia amenizar a situação, para que realmente a globalização seja vista apenas com fatores positivos, não somente para o país, mas sim para o bem-comum da sociedade\":0,\n",
        "    'os nazistas invadiram a polônia e criaram campos de concentração, matando diversos judeus':0,\n",
        "    'Eu gosto de pizza de pepperoni É a minha favorita':0,\n",
        "    'Todos são diferentes e isso é o que torna o mundo interessante':0,\n",
        "    'os nazistas mataram os judeus durante o Holocausto':0,\n",
        "    'os refugiados merecem respeito':0,\n",
        "    'o odio deve ser evitado':0,\n",
        "    'os gays sofrem muito preconceito':0,\n",
        "    'se eu fosse você não dava moral pra isso':0,\n",
        "    'não é por que o cara é comunista que ele merece morrer':0,\n",
        "    'O amor e o respeito devem ser dados a todas as religiões':0,\n",
        "    'Todos merecem igualdade de acesso aos recursos básicos, como água e comida':0,\n",
        "    'Com o decorrer do tempo, a globalização se tornou cada vez mais frequente no cotidiano da humanidade, havendo uma influência bastante significativa na vida das pessoas':0,\n",
        "    'pt é um partido de esquerda':0,\n",
        "    \"Gay é aquele que gosta do mesmo sexo\":0,\n",
        "    'O trabalho na primeira infância era visto nos primórdios da civilização como moldados de caráter':0,\n",
        "    'As crianças muitas vezes são privadas dos estudos para ajudar nas despesas':0,\n",
        "    'Isso gerará problemas futuros tanto no âmbito profissional como, de certa forma, no psicológico':0,\n",
        "    'Na revolução industrial, principalmente, no início do desenvolvimento fabril, o labor infantil era utilizado para diminuição de custos e arrecadação de um maior lucro':0,\n",
        "    'Atualmente, a situação não se difere dessa':0,\n",
        "    'Já que, devido a ineficiente fiscalização e aos baixos custos de contratação atrelados a dificuldades financeiras das famílias':0,\n",
        "    'Essa população infanto-juvenil é o brigada a trabalhar para prover a casa':0,\n",
        "    'E isso acarreta muitas vezes em um abandono escolar':0,\n",
        "    'Segundo o IBGE de 2015, mais de 2,7 milhões de jovens entre 5 e 17 anos de idade trabalham no país':0,\n",
        "    'Isso acarreta em um comprometimento congnitivo da criança':0,\n",
        "    'Bem como desencadeia em evasão acadêmica ou uma baixa performance escolar':0,\n",
        "    'Assim como também futuramente estarão sujeitos à empregos de baixa remuneração e ao desemprego, segundo um estudo da Organização Internacional do Trabalho (OIT)':0,\n",
        "    'Por isso, é indispensável que haja uma fiscalização eficiente por parte do Governo Federal e o Conselho Tutelar juntamente com a população para efetuar denúncias em casos de exploração infantil':0,\n",
        "    'Além de promover rodas de conversas e palestras para os pais incentivarem seus filhos a se manterem na escola, mesmo na idade de aprendiz':0,\n",
        "    \"Os gays merecem os mesmos direitos que nos\":0,\n",
        "    'O Grupo comunista M-19 foi massacrado pela mafia de extrema direita colombiana':0,\n",
        "    'Os negros são humanos e merecem respeito':0,\n",
        "    'O nazismo deve ser considerado pelo governo':1,\n",
        "    'Não podemos botar a culpa de todos os nossos problemas nos comunistas':0,\n",
        "    'Com o decorrer do tempo, a globalização se tornou cada vez mais frequente no cotidiano da humanidade, havendo uma influência bastante significativa na vida das pessoas':0,\n",
        "    'A globalização trouxe consigo muitos fatores positivos, taus como a integração econômica, social, cultural e política facilitando a interação com outros países pelo mundo':0,\n",
        "    'Porém, causou um grande descontentamento para muitos, pois impulsionou ao aumento da desigualdade social, e justamente por ser uma proposta que iria beneficiar a todos, acabou almejando a economia do país esquecendo da situação dos cidadãos que nele habitam':0,\n",
        "    'É de extrema importância um país se globalizar, contudo deve analisar a situação de determinados habitantes antes de efetivar \"negócios\" com outro país':0,\n",
        "    'No Brasil, há um alto índice de pessoas com extrema miséria, sem lugar para morar, sem comida para saciar o corpo, entre outros problemas enfrentados':0,\n",
        "    'Não seria possível combater esse descaso de uma única vez, porém com a contribuição de órgão públicos poderia amenizar a situação, para que realmente a globalização seja vista apenas com fatores positivos, não somente para o país, mas sim para o bem-comum da sociedade':0,\n",
        "    'Contudo conclui-se que a globalização é efetuada com o intúito de preservar o bem-estar e o progresso de um país, visando tornar-se uma fonte que preserve todas as, diretrizes necessárias para a manutenção do país e da sociedade contemporânea':0,\n",
        "    'Quando for debater mantenha a calma e não ataque a argumentadora oponente mas sim seus argumentos':0,\n",
        "}\n",
        "\n",
        "x=[]\n",
        "y=[]\n",
        "for k, v in a.items():\n",
        "    x.append(k.lower())\n",
        "    y.append(v)\n",
        "\n",
        "df = pd.DataFrame({'x':x, 'y':y})"
      ],
      "metadata": {
        "id": "AcHDs5cyd0SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessamento:"
      ],
      "metadata": {
        "id": "tk1DVDcmywW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "jhbm5d_CEsy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analise exploratória"
      ],
      "metadata": {
        "id": "ZWzLA0z9btkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data =data.drop(['Unnamed: 0'],axis=1)\n",
        "data['offensive_language'] = data['offensive_language'].replace({True: 1, False: 0})\n",
        "data"
      ],
      "metadata": {
        "id": "1TNjcHT2VQYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'x': 'instagram_comments'})\n",
        "df = df.rename(columns={'y': 'offensive_language'})\n",
        "df = pd.concat([df, data], ignore_index=True)\n",
        "df\n"
      ],
      "metadata": {
        "id": "FFgjuiz-xod2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x='offensive_language',data=df)"
      ],
      "metadata": {
        "id": "uwz8B--mbbpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pt_core_news_sm\n",
        "stop = pt_core_news_sm.load().Defaults.stop_words\n",
        "stop.add('pra')\n",
        "stop.add('ão')\n",
        "wordcloud = WordCloud(stopwords=stop).generate(' '.join(str(text) for text in df.instagram_comments.values.tolist()))\n",
        "\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-X00SzZ2cBcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXkhw3vCJUYf"
      },
      "source": [
        "# Implementação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm  # Importar o tqdm para a barra de progresso\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Carrega o modelo BERTimbau pré-treinado\n",
        "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Parâmetros de treinamento\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 8\n",
        "batch_size = 32\n",
        "dropout_rate = 0.1\n",
        "weight_decay = 0.01\n",
        "\n",
        "texts = df['instagram_comments'].tolist()\n",
        "labels = df['offensive_language'].tolist()\n",
        "\n",
        "\n",
        "normalized_texts = []\n",
        "normalized_labels = []\n",
        "\n",
        "for text, label in zip(texts, labels):\n",
        "    if isinstance(text, str):\n",
        "        # Remove pontuação e normaliza os textos\n",
        "        text = re.sub('[' + string.punctuation + ']', '', text)\n",
        "        normalized_texts.append(text.lower())\n",
        "        normalized_labels.append(label)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    normalized_texts, normalized_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    train_texts, truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
        ")\n",
        "test_encodings = tokenizer(\n",
        "    test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Cria o dataset\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels)\n",
        ")\n",
        "test_dataset = torch.utils.data.TensorDataset(\n",
        "    test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels)\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(\n",
        "    [\n",
        "        {'params': model.bert.parameters(), 'lr': learning_rate},\n",
        "        {'params': model.classifier.parameters(), 'lr': learning_rate}\n",
        "    ],\n",
        "    weight_decay=weight_decay#penaliza pesos maiores serve para (basicamente L2): Prevenção de overfitting, Controle da complexidade do modelo\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Move o modelo para a GPU, se disponível\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "for param in model.bert.encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "for module in model.bert.encoder.layer:\n",
        "    module.output.dropout.p = dropout_rate\n",
        "\n",
        "# Função de treinamento\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc='Training')  # Barra de progresso\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix({'Loss': loss.item()})  # Atualizar o valor da perda na barra de progresso\n",
        "\n",
        "    train_loss /= len(loader)\n",
        "    accuracy = total_correct / len(loader.dataset)\n",
        "\n",
        "    return train_loss, accuracy\n",
        "\n",
        "# Função de avaliação\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc='Evaluation')  # Barra de progresso\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            progress_bar.set_postfix({'Loss': loss.item()})  # Atualizar o valor da perda na barra de progresso\n",
        "\n",
        "        eval_loss /= len(loader)\n",
        "        accuracy = total_correct / len(loader.dataset)\n",
        "\n",
        "    return eval_loss, accuracy\n",
        "\n",
        "# Agendamento de taxa de aprendizagem\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=1, verbose=True\n",
        ")\n",
        "\n",
        "# Treinamento do modelo\n",
        "best_accuracy = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion)\n",
        "    eval_loss, eval_accuracy = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Eval Loss: {eval_loss:.4f} - Eval Accuracy: {eval_accuracy:.4f}')\n",
        "\n",
        "    scheduler.step(eval_loss)\n",
        "\n",
        "    if eval_accuracy > best_accuracy:\n",
        "        best_accuracy = eval_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "# Carregar o modelo com a melhor precisão no conjunto de teste\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "# Previsões\n",
        "def predict(model, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids, attention_mask, _ = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            predictions.extend(predicted.cpu().tolist())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "    \n",
        "\n",
        "test_predictions = predict(model, test_loader)\n",
        "\n",
        "print(classification_report(test_labels, test_predictions))\n"
      ],
      "metadata": {
        "id": "dHHXWtQMattQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'best_model.pt')"
      ],
      "metadata": {
        "id": "iY53HjVsDHUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes:"
      ],
      "metadata": {
        "id": "YurapDPd6mqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "NUBS8KZ68nlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4d7cfd-cb36-41f2-80ee-c2e5e8061dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm  # Importar o tqdm para a barra de progresso\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def predict(model, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            batch_predictions = logits.argmax(dim=1).cpu().tolist()\n",
        "            predictions.extend(batch_predictions)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Specify the device as CPU\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Load the saved model and map it to the CPU\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "loaded_model.load_state_dict(torch.load('/content/drive/MyDrive/best_model8.pt', map_location=device))\n",
        "loaded_model.to(device)\n",
        "\n",
        "# Preparar os dados para a previsão\n",
        "input_texts = [\"Morre preto fudido\", 'Os imigrantes não deveriam ser impedidos de entrar no meu país','os imigrantes deveriam ser impedidos de entrar no meu país',\"Bandido bom é bandido morto\",'os comunistas são pessoas como nos e merecem respeito','O Grupo comunista M-19 foi massacrado pela mafia de extrema direita colombiana', 'A cada dia fico mais admirado com a cara de pau da elite dominante desse mundo até quando irão nos fazer de otários','Aquele cara é um babaca','quem não deve não teme','eu te amo','Podemos concluir que a terra é redonda', 'matem os judeo', 'Todo indígena é corrupto', 'hoje foi há loja de Dona maria comprar pão quando derepente']\n",
        "\n",
        "\n",
        "input_encodings = tokenizer(\n",
        "    input_texts, truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
        ")\n",
        "input_dataset = torch.utils.data.TensorDataset(\n",
        "    input_encodings['input_ids'], input_encodings['attention_mask']\n",
        ")\n",
        "input_loader = torch.utils.data.DataLoader(\n",
        "    input_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "# Fazer previsões\n",
        "predictions = predict(loaded_model, input_loader)\n",
        "\n",
        "# Imprimir as previsões\n",
        "for text, prediction in zip(input_texts, predictions):\n",
        "    print(f'Texto: {text} - Previsão: {prediction}')\n"
      ],
      "metadata": {
        "id": "9xQuntzMa1wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c99803-f51c-49e8-9d1d-eebb14246a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: Morre preto fudido - Previsão: 1\n",
            "Texto: Os imigrantes não deveriam ser impedidos de entrar no meu país - Previsão: 0\n",
            "Texto: os imigrantes deveriam ser impedidos de entrar no meu pais - Previsão: 1\n",
            "Texto: Bandido bom é bandido morto - Previsão: 1\n",
            "Texto: os comunistas são pessoas como nos e merecem respeito - Previsão: 0\n",
            "Texto: O Grupo comunista M-19 foi massacrado pela mafia de extrema direita colombiana - Previsão: 1\n",
            "Texto: A cada dia fico mais admirado com a cara de pau da elite dominante desse mundo até quando irão nos fazer de otários - Previsão: 1\n",
            "Texto: Aquele cara é um babaca - Previsão: 1\n",
            "Texto: quem não deve não teme - Previsão: 0\n",
            "Texto: eu te amo - Previsão: 0\n",
            "Texto: Podemos concluir que a terra é redonda - Previsão: 0\n",
            "Texto: matem os judeo - Previsão: 1\n",
            "Texto: Todo indígena é corrupto - Previsão: 1\n",
            "Texto: hoje foi há loja de Dona maria comprar pão quando derepente - Previsão: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 'Querido filho,\\n Fiquei sabendo que você terá seu primeiro debate fudido na próxima semana. Eu já conheço bem suas ideias antifeministas e sei que a feminista que debaterá com você é extremista. Sendo seu pai, sinto que é meu dever te lhe dar alguns conselhos.\\n Quando for debater, mantenha a calma e não ataque a argumentadora oponente, mas sim seus argumentos. Esse é o primeiro passo para que o debate seja civilizado e produtivo. Use a razão e a lógica. Caso a feminista faça o oposto disso, e, por exemplo, descarte os seus argumentos meramente por você ser homem, branco, heterossexual e cristão, explique ao público que ela está usando a famosa falácia ad hominem.\\n Evite e aponte, meu querido filho, todas as falácias lógicas. Falácias destroem o bom debate. Caso a feminista se esquive das críticas contra o feminismo, afirmando que aquilo que está sendo criticado por você chama-se \"femismo\" e não \"feminismo\", avise-a que ela está utilizando a falácia do escocês de verdade.\\n  \\n Mantenha-se firme também na defesa pela liberdade de expressão. Essa é outra regra de ouro para o bom debate. Não importa o quão verbalmente agressiva seja a feminista ou quão repulsivas pareçam ser suas ideias, lembre-se que ela possui o direito de falar tudo aquilo que realmente pensa. Se tudo o que ela fala for de fato tão horrível e irracional assim, então será fácil para você desconstruir publicamente os argumentos dela. Lembre-se daquilo que sempre te ensinei: tentar calar o debatedor adversário à força, como se estivesse cometendo um crime por expor o que pensa, é para aqueles que não conseguem convencer os demais através do uso da lógica e da razão, então precisam fazer isso através de meios coercitivos.\\n Portanto, filho, se alguém nesse debate utilizar constantemente argumentos falaciosos ou tentar calar à força o adversário, alegando estar combatendo um \"discurso de ódio\", então que esse alguém não seja você. Não podemos controlar a feminista com quem você debaterá, mas você tem controle sobre si próprio. Faça a sua parte e tenha um bom debate !\\n Com carinho,\\n Seu amado Pai.'\n",
        "a = a.replace('\\n', '')\n",
        "a = a.split('.')\n",
        "\n",
        "\n",
        "# Preparar os dados para a previsão\n",
        "input_encodings = tokenizer(\n",
        "    a, truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
        ")\n",
        "input_dataset = torch.utils.data.TensorDataset(\n",
        "    input_encodings['input_ids'], input_encodings['attention_mask']\n",
        ")\n",
        "input_loader = torch.utils.data.DataLoader(\n",
        "    input_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "predictions = predict(loaded_model, input_loader)\n",
        "\n",
        "for text, prediction in zip(a, predictions):\n",
        "    print(f'Texto: {text} - Previsão: {prediction}')\n"
      ],
      "metadata": {
        "id": "WTcsxLwU6ioA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222fbd27-62e9-48db-b3e3-e9316a3678aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: Querido filho, Fiquei sabendo que você terá seu primeiro debate fudido na próxima semana - Previsão: 0\n",
            "Texto:  Eu já conheço bem suas ideias antifeministas e sei que a feminista que debaterá com você é extremista - Previsão: 0\n",
            "Texto:  Sendo seu pai, sinto que é meu dever te lhe dar alguns conselhos - Previsão: 0\n",
            "Texto:  Quando for debater, mantenha a calma e não ataque a argumentadora oponente, mas sim seus argumentos - Previsão: 0\n",
            "Texto:  Esse é o primeiro passo para que o debate seja civilizado e produtivo - Previsão: 0\n",
            "Texto:  Use a razão e a lógica - Previsão: 0\n",
            "Texto:  Caso a feminista faça o oposto disso, e, por exemplo, descarte os seus argumentos meramente por você ser homem, branco, heterossexual e cristão, explique ao público que ela está usando a famosa falácia ad hominem - Previsão: 0\n",
            "Texto:  Evite e aponte, meu querido filho, todas as falácias lógicas - Previsão: 0\n",
            "Texto:  Falácias destroem o bom debate - Previsão: 0\n",
            "Texto:  Caso a feminista se esquive das críticas contra o feminismo, afirmando que aquilo que está sendo criticado por você chama-se \"femismo\" e não \"feminismo\", avise-a que ela está utilizando a falácia do escocês de verdade - Previsão: 0\n",
            "Texto:    Mantenha-se firme também na defesa pela liberdade de expressão - Previsão: 0\n",
            "Texto:  Essa é outra regra de ouro para o bom debate - Previsão: 0\n",
            "Texto:  Não importa o quão verbalmente agressiva seja a feminista ou quão repulsivas pareçam ser suas ideias, lembre-se que ela possui o direito de falar tudo aquilo que realmente pensa - Previsão: 0\n",
            "Texto:  Se tudo o que ela fala for de fato tão horrível e irracional assim, então será fácil para você desconstruir publicamente os argumentos dela - Previsão: 1\n",
            "Texto:  Lembre-se daquilo que sempre te ensinei: tentar calar o debatedor adversário à força, como se estivesse cometendo um crime por expor o que pensa, é para aqueles que não conseguem convencer os demais através do uso da lógica e da razão, então precisam fazer isso através de meios coercitivos - Previsão: 0\n",
            "Texto:  Portanto, filho, se alguém nesse debate utilizar constantemente argumentos falaciosos ou tentar calar à força o adversário, alegando estar combatendo um \"discurso de ódio\", então que esse alguém não seja você - Previsão: 0\n",
            "Texto:  Não podemos controlar a feminista com quem você debaterá, mas você tem controle sobre si próprio - Previsão: 0\n",
            "Texto:  Faça a sua parte e tenha um bom debate ! Com carinho, Seu amado Pai - Previsão: 0\n",
            "Texto:  - Previsão: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = ['seu comunista','eu te odeio do fundo do meu coração seu chato', 'o odio deve ser evitado', 'por mim agente mandava de F todos esse refugiados','os refugiados merecem respeito', 'por mim agente acabava logo com essa modinha de ser gay e voltava tudo ao normal', 'os gays sofreram muito preconceito ao longo da história']\n",
        "\n",
        "\n",
        "# Preparar os dados para a previsão\n",
        "input_encodings = tokenizer(\n",
        "    b, truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
        ")\n",
        "input_dataset = torch.utils.data.TensorDataset(\n",
        "    input_encodings['input_ids'], input_encodings['attention_mask']\n",
        ")\n",
        "input_loader = torch.utils.data.DataLoader(\n",
        "    input_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "predictions = predict(loaded_model, input_loader)\n",
        "\n",
        "for text, prediction in zip(b, predictions):\n",
        "    print(f'Texto: {text} - Previsão: {prediction}')\n"
      ],
      "metadata": {
        "id": "L6xwjioEMXHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70494b7-18bf-4f8a-fcc4-374634252f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: seu comunista - Previsão: 1\n",
            "Texto: eu te odeio do fundo do meu coração seu chato - Previsão: 1\n",
            "Texto: o odio deve ser evitado - Previsão: 1\n",
            "Texto: por mim agente mandava de F todos esse refugiados - Previsão: 1\n",
            "Texto: os refugiados merecem respeito - Previsão: 0\n",
            "Texto: por mim agente acabava logo com essa modinha de ser gay e voltava tudo ao normal - Previsão: 1\n",
            "Texto: os gays sofreram muito preconceito ao longo da história - Previsão: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = ['Todo europeu é xenofóbico', 'a xenofóbia deve ser evitada','A russia é superior aos Estados Unidos por ser comunista ']\n",
        "\n",
        "\n",
        "# Preparar os dados para a previsão\n",
        "input_encodings = tokenizer(\n",
        "    b, truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
        ")\n",
        "input_dataset = torch.utils.data.TensorDataset(\n",
        "    input_encodings['input_ids'], input_encodings['attention_mask']\n",
        ")\n",
        "input_loader = torch.utils.data.DataLoader(\n",
        "    input_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "predictions = predict(loaded_model, input_loader)\n",
        "\n",
        "for text, prediction in zip(b, predictions):\n",
        "    print(f'Texto: {text} - Previsão: {prediction}')\n"
      ],
      "metadata": {
        "id": "L8ZzWAbVkiqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2971b79c-ca75-405d-a96f-81e87fd6235a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: Todo europeu é xenofóbico - Previsão: 1\n",
            "Texto: a xenofóbia deve ser evitada - Previsão: 0\n",
            "Texto: A russia é superior aos Estados Unidos por ser comunista  - Previsão: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_string(text):\n",
        "        \"\"\"\n",
        "        Verifica se uma string é válida, ou seja, não é composta somente por espaços em branco,\n",
        "        pontuações ou numerais.\n",
        "\n",
        "        :param string: A string a ser verificada.\n",
        "        :type string: str\n",
        "        :return: Retorna True se a string é válida e False caso contrário.\n",
        "        :rtype: bool\n",
        "        \"\"\"\n",
        "        if text.strip() and not all(char.isdigit() or char.isspace() or char in string.punctuation for char in text):\n",
        "            return True\n",
        "        return False\n",
        "class TokenCRIA():\n",
        "    \"\"\"\n",
        "    Classe para padronizar o acesso a informações de tokens no formato CRIA\n",
        "\n",
        "    Atributos:\n",
        "    ----------\n",
        "        texto : str\n",
        "            Texto para compor Token\n",
        "        span : tuple\n",
        "            Tupla contendo posição inicial e final do texto na string original\n",
        "    \"\"\"\n",
        "    def __init__(self, palavra, span):\n",
        "        self.text = str(palavra)\n",
        "        self.start = int(span[0])\n",
        "        self.end = int(span[1])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.text\n",
        "\n",
        " \n",
        "\n",
        "def divideSentencas(texto):\n",
        "        \"\"\"\n",
        "        Divide as sentenças de um texto.\n",
        "        ...\n",
        "\n",
        "        Atributos:\n",
        "        ---------\n",
        "            texto : str\n",
        "                String a ser dividida\n",
        "\n",
        "        Retorna:\n",
        "        ---------\n",
        "                list[TokenCRIA] : list\n",
        "                    Lista de TokenCRIA com sentenças do texto.\n",
        "        \"\"\"\n",
        "        sentence_separator = '.?!'\n",
        "        lista_expressoes_abreviadas = [\n",
        "            r'\\d',r'Art',r'Lei',r'N',r'Dr',r'Dra',\n",
        "            r'Sr',r'Sra',r'Av',r'Sec',r'Apt']\n",
        "        negative_lookbehind = '?<!'\n",
        "        regular_exceptions = ''.join([f'({negative_lookbehind}{expressao})' for expressao in lista_expressoes_abreviadas])\n",
        "        sentence_endings_pattern = f\"{regular_exceptions}([{sentence_separator}])\"\n",
        "        splitStep = re.split(sentence_endings_pattern, texto)\n",
        "        splitedList = [splitStep[index]+splitStep[index+1] for index in range(0,len(splitStep),2) if index!= len(splitStep)-1]\n",
        "        sentences = [sentenca for sentenca in splitedList if is_valid_string(sentenca)]\n",
        "        pos_finais = [texto.index(sentenca) + len(sentenca) for sentenca in sentences]\n",
        "        pos_iniciais = [texto.index(sentenca) for sentenca in sentences]\n",
        "        \n",
        "        return [TokenCRIA(sentenca, (pos_iniciais[i], pos_finais[i])) for i, sentenca in enumerate(sentences)]\n",
        "a = divideSentencas('ontem eu estava andando quando derepente fui pego de surpresa! havia um sapo atraz de mim.')\n",
        "print(a)"
      ],
      "metadata": {
        "id": "bJ4wl4O7xIZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e74d13-8d0e-4a57-b578-b8e715e428b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ontem eu estava andando quando derepente fui pego de surpresa!,  havia um sapo atraz de mim.]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tk1DVDcmywW8"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "efadc5051de448e091cd76755371cc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a220fe561e054614a819821e35ad3d3d",
              "IPY_MODEL_3f49fcb4d79c4a2599573d64b2466140",
              "IPY_MODEL_0a8b6d3a3b9c48a5878b58c2f3933f69",
              "IPY_MODEL_5c2f22e1d46c4c16a0c25ff0d6c726b4",
              "IPY_MODEL_bc83b01ddd144e38bff29ae0920cc98a"
            ],
            "layout": "IPY_MODEL_effe990ecf06444ba81f873b290ddc37"
          }
        },
        "a220fe561e054614a819821e35ad3d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5b60b77b8d45e79651b365a69499be",
            "placeholder": "​",
            "style": "IPY_MODEL_273c382d7a88488197e0565aca12f90e",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3f49fcb4d79c4a2599573d64b2466140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5aa07aa277b14a7da279aa0e820ee83f",
            "placeholder": "​",
            "style": "IPY_MODEL_3ff8747619a040e8ae5bcbab7b6adab4",
            "value": ""
          }
        },
        "0a8b6d3a3b9c48a5878b58c2f3933f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_d8d1be78d2274c6d8b53c2382f953bd5",
            "style": "IPY_MODEL_f2f2a37e44194ee9ab4cad389a29e3a1",
            "value": true
          }
        },
        "5c2f22e1d46c4c16a0c25ff0d6c726b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_672e003592a843779aa02ae363c361e0",
            "style": "IPY_MODEL_e9059b7cfe834b5c82d2a168d406f832",
            "tooltip": ""
          }
        },
        "bc83b01ddd144e38bff29ae0920cc98a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45d9e8546b964905bdcec026f15aa2b7",
            "placeholder": "​",
            "style": "IPY_MODEL_3f3e976f3ba04f91a61459e91d9ff5b6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "effe990ecf06444ba81f873b290ddc37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "be5b60b77b8d45e79651b365a69499be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273c382d7a88488197e0565aca12f90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aa07aa277b14a7da279aa0e820ee83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ff8747619a040e8ae5bcbab7b6adab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8d1be78d2274c6d8b53c2382f953bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2f2a37e44194ee9ab4cad389a29e3a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "672e003592a843779aa02ae363c361e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9059b7cfe834b5c82d2a168d406f832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "45d9e8546b964905bdcec026f15aa2b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f3e976f3ba04f91a61459e91d9ff5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
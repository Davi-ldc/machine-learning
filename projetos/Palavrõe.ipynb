{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHrRmI46eDhp"
      },
      "source": [
        "1 - Objetivo do Modelo\n",
        "\n",
        "2 - Como obteve os Dados e quais informações temos sobre esse dataset?\n",
        "\n",
        "3 - Quais as etapas você usou para modelagem (como escolheu o modelo, quais metricas usou, o que fez para processar os dados) ?\n",
        "\n",
        "4 - Resultados experimentais da modelagem (quanto acerta? Falsos positivos? Verdadeiros Negativos?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1LdUpERMLuF"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LtNlgqQo3FT",
        "outputId": "ea715ce8-df36-4171-b246-293e84ab1b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting snscrape\n",
            "  Downloading snscrape-0.6.2.20230320-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from snscrape) (2.27.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from snscrape) (3.12.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->snscrape) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.6.2.20230320\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_cleaner\n",
            "  Downloading spacy_cleaner-3.1.3-py3-none-any.whl (15 kB)\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from spacy_cleaner)\n",
            "  Downloading spacy-3.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-lookups-data<1.1.0,>=1.0.3 (from spacy_cleaner)\n",
            "  Downloading spacy_lookups_data-1.0.3-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm<4.65.0,>=4.64.0 (from spacy_cleaner)\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (8.1.9)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.1->spacy_cleaner)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy_cleaner) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->spacy_cleaner) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->spacy_cleaner) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->spacy_cleaner) (2.1.2)\n",
            "Installing collected packages: wasabi, tqdm, spacy-lookups-data, spacy, spacy_cleaner\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.4.4 spacy-lookups-data-1.0.3 spacy_cleaner-3.1.3 tqdm-4.64.1 wasabi-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting syllables\n",
            "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
            "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0 (from syllables)\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /usr/local/lib/python3.10/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.15.0)\n",
            "Installing collected packages: importlib-metadata, cmudict, syllables\n",
            "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 syllables-1.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install snscrape\n",
        "!pip install spacy_cleaner\n",
        "!pip install syllables\n",
        "!pip install demoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BIF-CPPcNgw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import string\n",
        "import spacy_cleaner\n",
        "import demoji\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import snscrape.modules.twitter as SnTwitter\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCkXQuxKFYkS"
      },
      "source": [
        "# palavrões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq845esh_78n",
        "outputId": "fe27c3c0-7471-4dc6-d156-ac2d94ec86a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "animal de teta\n",
            "['arrombado', 'babaca', 'baitola', 'bicha', 'bosta', 'buceta', 'cacete', 'caralho', 'cu', 'debilmental', 'demente', 'desgracado', 'enfianocu', 'escroto', 'fdp', 'foda', 'fuder', 'fudido', 'fudeo', 'fodasse', 'gozado', 'idiota', 'imbecil', 'imundo', 'ku', 'limpezaanal', 'lixo', 'miolodecu', 'montedemerda', 'olhodocu', 'otario', 'palhaco', 'piroca', 'porra', 'punheta', 'puta', 'putaquepariu', 'pqprapariga', 'retardado', 'vadia', 'vagabundo', 'vagabunda', 'viado', 'xereca', 'arrombado', 'baba-ovo', 'babaca', 'babaovo', 'baitola', 'bicha', 'bixa', 'boceta', 'buceta', 'boiola', 'bokete', 'boquete', 'bosseta', 'bosta', 'buceta', 'caceta', 'cacete', 'canalha', 'caralho', 'casseta', 'cassete', 'checheca', 'chereca', 'chifruda', 'chifrudo', 'corna', 'cornagem', 'cornisse', 'corno', 'cornuda', 'cornudo', 'cornão', 'cú', 'curalho', 'cuzao', 'cuzão', 'cuzuda', 'cuzudo', 'egua', 'égua', 'estupida', 'estúpida', 'estupidez', 'estupido', 'estúpido', 'foda', 'fodao', 'fodão', 'fode', 'fodi', 'fodida', 'fodido', 'fudendo', 'fudeção', 'fudida', 'fudido', 'iscrota', 'iscroto', 'masturba', 'merda', 'mongoloide', 'mongolóide', 'pica', 'picao', 'picão', 'pinto', 'pintudo', 'pintão', 'piroca', 'porra', 'punheta', 'punhetao', 'punhetão', 'puta', 'puto', 'retardada', 'retardado', 'siririca', 'vadia', 'vagabunda', 'vagabundo', 'vagina', 'veadao', 'viada', 'viadagem', 'viadao', 'viadão', 'viado', 'viadão', 'víado', 'xerereca', 'xexeca']\n"
          ]
        }
      ],
      "source": [
        "palavrao = [\n",
        "  'Arrombado', 'Babaca', 'Baitola', 'Bicha', 'Bosta', 'Buceta', 'Cacete', 'Caralho', 'Cu', 'DebilMental', 'Demente', 'Desgracado', 'Enfianocu', 'Escroto', 'FDP', 'Foda', 'Fuder', 'Fudido', 'fudeo', 'fodasse',\n",
        " 'Gozado', 'Idiota', 'Imbecil', 'Imundo', 'Ku', 'Limpezaanal', 'lixo',\n",
        " \n",
        " 'MiolodeCu', \n",
        " 'MontedeMerda',\n",
        " 'Olhodocu',\n",
        " 'Otario', 'Palhaco',\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        " 'Piroca',\n",
        " 'Porra',\n",
        " 'Punheta', 'Puta', 'PutaQuePariu', 'pqp'\n",
        " \n",
        " 'Rapariga',\n",
        " 'Retardado',\n",
        " \n",
        "\n",
        " 'Vadia', 'Vagabundo', 'vagabunda',\n",
        " \n",
        " 'Viado', \n",
        " 'Xereca',\n",
        " \n",
        " \n",
        " 'arrombado', \n",
        " 'baba-ovo', 'babaca', 'babaovo',\n",
        " 'baitola', \n",
        " 'bicha', 'bixa',\n",
        " 'boceta', 'buceta', \n",
        " 'boiola', 'bokete', \n",
        " 'boquete', 'bosseta', 'bosta', \n",
        " 'buceta', \n",
        " \n",
        " 'caceta', 'cacete', \n",
        " 'canalha',\n",
        " 'caralho', 'casseta', 'cassete', \n",
        " 'checheca', 'chereca',\n",
        " 'chifruda', 'chifrudo', \n",
        " 'corna', 'cornagem', 'cornisse', 'corno', 'cornuda', 'cornudo', 'cornão',\n",
        " 'cú',\n",
        " 'curalho', 'cuzao', 'cuzão', 'cuzuda', 'cuzudo', \n",
        " 'egua', 'égua', \n",
        " 'estupida', 'estúpida', 'estupidez', 'estupido', 'estúpido',\n",
        " 'foda', 'fodao', 'fodão', 'fode', 'fodi', 'fodida', 'fodido',\n",
        " 'fudendo', 'fudeção', 'fudida', 'fudido', \n",
        " 'iscrota', 'iscroto', \n",
        " 'masturba', \n",
        " 'merda',\n",
        " 'mongoloide', 'mongolóide',\n",
        "\n",
        "\n",
        " 'pica', 'picao', 'picão', \n",
        " 'pinto', 'pintudo', 'pintão', \n",
        " 'piroca',\n",
        "\n",
        "\n",
        " 'porra',\n",
        " 'punheta', 'punhetao', 'punhetão',\n",
        " 'puta', 'puto',\n",
        " 'retardada', 'retardado', 'siririca', 'vadia', 'vagabunda', 'vagabundo', 'vagina', 'veadao', 'viada', 'viadagem', 'viadao', 'viadão', 'viado', 'viadão', 'víado', 'xerereca', 'xexeca',\n",
        "]\n",
        "translator = str.maketrans('','',string.punctuation)\n",
        "\n",
        "for i,c in enumerate(palavrao):\n",
        "    palavrao[i] =c.translate(translator)\n",
        "    palavrao[i] = c.lower()\n",
        "\n",
        "\n",
        "a = 'Animal de Teta'\n",
        "print(a.lower())\n",
        "\n",
        "\n",
        "print(palavrao)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H11wiPj6HLMO"
      },
      "source": [
        "#scrapping dos dados (não a necessidade de rodar, apenas fassa o upload dos dado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzWApSA1xZSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "b6fda706-cfdf-4588-bca0-89a6b99f71c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:snscrape.base:Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=lang%3Apt&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
            "CRITICAL:snscrape.base:4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=lang%3Apt&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
            "CRITICAL:snscrape.base:Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ScraperException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-04745d5826f1>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mc_palavrão\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mc_sem_palavrão\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_palavrão\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_sem_palavrão\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mc_palavrão\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m9000\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc_sem_palavrão\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cursor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://api.twitter.com/2/search/adaptive.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_timeline_instructions_to_tweets_or_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapiType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mapiType\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPHQL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote_via\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_api_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    245\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Errors: {\", \".join(errors)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mScraperException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reached unreachable code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=lang%3Apt&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
          ]
        }
      ],
      "source": [
        "# palavrao = re.compile(r'((?i)\\bm[e3]rd\\S+\\b)|(?i)\\bb[o0]st\\S+\\b|(?i)\\bput[^r]\\S*\\b|(?i)\\bp[o0]rr[a@4]\\S*\\b|(?i)\\bc[uú](\\b|z\\w+)|(?i)\\bv[i1](nh)?[a@4]d[^u]\\S*\\b|(?i)\\b[kc][o0a@4]?r[o0a@4]?l[hi]?[o0a@4]?\\S*\\b|(?i)\\bf[o0u]d\\S+\\b|\\b[fF](\\.)?[dD](\\.)?[pP]\\b|\\b[Pp](\\.)?[Qq](\\.)?[pP]\\b|\\b[Vv][Ss][Ff]\\b|\\b[Vv][Tt][Nn][Cc]\\b|(?i)b[o0u]c[e3]t\\S+\\b|(?i)\\bpunh[e3]t\\S+\\b|(?i)\\bb[i1](ch|x)[a@4]s?\\b|\\b(?i)c[o0]c[0ô]s?\\b|\\b(?i)[e3]scr[o0]t\\S+\\b|(?i)\\bb[a@4]b[a@4][qc]\\S+\\b|(?i)\\bc[a@4]g\\S+\\b|(?i)\\bs[a@4]c[a@4]n[a@4e31i]\\S*\\b|(?i)\\bk[a@4]c[e3]?t[e3]?\\S*\\b')\n",
        "\n",
        "\n",
        "\n",
        "scrapper = SnTwitter.TwitterSearchScraper('lang:pt')\n",
        "\n",
        "\n",
        "\n",
        "x = []\n",
        "y = [] \n",
        "\n",
        "c_palavrão = 0 \n",
        "c_sem_palavrão = 0\n",
        "for i, tweet in enumerate(scrapper.get_items()):\n",
        "  print(c_palavrão, c_sem_palavrão)\n",
        "  if c_palavrão > 9000 and c_sem_palavrão > 10000:\n",
        "    break\n",
        "\n",
        "  new_tweet = tweet.rawContent.split(' ')\n",
        "\n",
        "\n",
        "      \n",
        "  new_tweet = new_tweet.replace('\\n', '')\n",
        "  new_tweet = re.sub(r'\\B#\\w+', '', new_tweet)#hastag\n",
        "  new_tweet = demoji.replace(new_tweet, '') #emojis\n",
        "  new_tweet = re.sub(r'http\\S+', '', new_tweet)#links\n",
        "\n",
        "  # print(new_tweet)\n",
        "  zx = False\n",
        "  for c in new_tweet.split():\n",
        "    # print(new_tweet.split())\n",
        "    if c in palavrao:\n",
        "\n",
        "       zx = True\n",
        "  if zx: #o regex não inclue essa palavra\n",
        "    print('oi')\n",
        "    if c_palavrão > 9000:\n",
        "      continue\n",
        "\n",
        "\n",
        "      \n",
        "    x.append(new_tweet.lower())\n",
        "    y.append(1)\n",
        "    c_palavrão +=1\n",
        "  \n",
        "  else:\n",
        "    if c_sem_palavrão > 10000:\n",
        "      continue\n",
        "    if tweet.lang != \"pt\" and tweet.lang != 'pt-br':\n",
        "      continue\n",
        "\n",
        "    x.append(new_tweet.lower())\n",
        "    y.append(0)\n",
        "    c_sem_palavrão +=1\n",
        "print(x)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm0s1D2vHVdB"
      },
      "outputs": [],
      "source": [
        "# d = {'frases': x, 'palavrão': y}\n",
        "# df = pd.DataFrame(data=d)\n",
        "# df.to_csv('output.csv')\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/df_palavroesv2.csv')\n",
        "\n",
        "# df = pd.concat([df1, df2], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM1qzD2Ddq98"
      },
      "source": [
        "#  preprocessamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0nKqNDtl3RG"
      },
      "outputs": [],
      "source": [
        "#roda se o erro \n",
        "#\"NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\" \n",
        "#aparecer durante a instalação do pt_core_news_sm\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lssdd5Ho7yCP",
        "outputId": "a9e582a3-8493-4ce8-cc73-e4748751b27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-29 13:17:23.619802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.1.2)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3PVUFI1ej20"
      },
      "source": [
        "Remove o vies dos dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxsBDIWZtgEz"
      },
      "outputs": [],
      "source": [
        "df['frases'].isnull().sum()\n",
        "df =df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdKOK9h6cVbX",
        "outputId": "11294e7e-4d8d-4330-f95b-ea71a408b6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b99c229d918d>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.frases[i] = df.frases[i].replace('lula', np.random.choice([\"Alice\", \"Breno\", \"Carmem\", \"Dalva\", \"Edson\", \"Felipe\", \"Gabriela\", \"Henrique\", \"Igor\", \"Júlia\", \"Karla\", \"Lara\", \"Márcio\", \"Nina\", \"Olívia\", \"Pablo\", \"Ricardo\", \"Sara\", \"Tânia\", \"Ubirajara\", \"Valentina\", \"Wagner\", \"Xavier\", \"Yago\", \"Zélia\"]))\n",
            "<ipython-input-9-b99c229d918d>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.frases[i] = df.frases[i].replace('Brasil', np.random.choice([\"Angola\", \"Benin\", \"Brasil\", \"Chade\", \"Cuba\", \"Dinamarca\", \"Fiji\", \"Gana\", \"Haiti\", \"Índia\", \"Japão\", \"Jordânia\", \"Kuwait\", \"Lesoto\", \"Malta\", \"México\", \"Moçambique\", \"Níger\", \"Panamá\", \"Paraguai\", \"Peru\", \"Polônia\", \"Qatar\", \"Ruanda\", \"Síria\", \"Sudão\", \"Suriname\", \"Tunísia\", \"Turquia\", \"Uganda\", \"Uruguai\", \"Vanuatu\"]\n",
            "<ipython-input-9-b99c229d918d>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.frases[i] = df.frases[i].replace('lulaoficial', np.random.choice([\"Alice\", \"Breno\", \"Carmem\", \"Dalva\", \"Edson\", \"Felipe\", \"Gabriela\", \"Henrique\", \"Igor\", \"Júlia\", \"Karla\", \"Lara\", \"Márcio\", \"Nina\", \"Olívia\", \"Pablo\", \"Ricardo\", \"Sara\", \"Tânia\", \"Ubirajara\", \"Valentina\", \"Wagner\", \"Xavier\", \"Yago\", \"Zélia\"]))\n"
          ]
        }
      ],
      "source": [
        "import pt_core_news_sm\n",
        "#substitue o nome lulaoficial por nomes aleteatorios e cria uma nova coluna com a pessoa da frase\n",
        "nlp = pt_core_news_sm.load()\n",
        "for i,c in enumerate(df['frases']):\n",
        "  # print(i)\n",
        "  if 'lulaoficial' in c or 'lula' in c or 'brasil' in c:\n",
        "    df.frases[i] = df.frases[i].replace('lulaoficial', np.random.choice([\"Alice\", \"Breno\", \"Carmem\", \"Dalva\", \"Edson\", \"Felipe\", \"Gabriela\", \"Henrique\", \"Igor\", \"Júlia\", \"Karla\", \"Lara\", \"Márcio\", \"Nina\", \"Olívia\", \"Pablo\", \"Ricardo\", \"Sara\", \"Tânia\", \"Ubirajara\", \"Valentina\", \"Wagner\", \"Xavier\", \"Yago\", \"Zélia\"]))\n",
        "    df.frases[i] = df.frases[i].replace('lula', np.random.choice([\"Alice\", \"Breno\", \"Carmem\", \"Dalva\", \"Edson\", \"Felipe\", \"Gabriela\", \"Henrique\", \"Igor\", \"Júlia\", \"Karla\", \"Lara\", \"Márcio\", \"Nina\", \"Olívia\", \"Pablo\", \"Ricardo\", \"Sara\", \"Tânia\", \"Ubirajara\", \"Valentina\", \"Wagner\", \"Xavier\", \"Yago\", \"Zélia\"]))\n",
        "    df.frases[i] = df.frases[i].replace('Brasil', np.random.choice([\"Angola\", \"Benin\", \"Brasil\", \"Chade\", \"Cuba\", \"Dinamarca\", \"Fiji\", \"Gana\", \"Haiti\", \"Índia\", \"Japão\", \"Jordânia\", \"Kuwait\", \"Lesoto\", \"Malta\", \"México\", \"Moçambique\", \"Níger\", \"Panamá\", \"Paraguai\", \"Peru\", \"Polônia\", \"Qatar\", \"Ruanda\", \"Síria\", \"Sudão\", \"Suriname\", \"Tunísia\", \"Turquia\", \"Uganda\", \"Uruguai\", \"Vanuatu\"]\n",
        "))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNHurOMv475e"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "class PLNprocess():\n",
        "  def __init__(self):\n",
        "    self.pln = pt_core_news_sm.load()\n",
        "    self.stop_words = self.pln.Defaults.stop_words\n",
        "    self.pontuacoes = string.punctuation\n",
        "    return\n",
        "\n",
        "  def preprocessamento(self,text):\n",
        "    result = []\n",
        "    \n",
        "    texto = re.sub(r'[^a-zA-z0-9áéíóúÁÉÍÓÚàèìòùÀÈÌÒÙÃãõÕçÇ: ]','',text)\n",
        "    doc = self.pln(texto.lower())\n",
        "    \n",
        "    for token in doc:\n",
        "      \n",
        "      if(token.text in self.stop_words or token.text in self.pontuacoes ):\n",
        "        continue\n",
        "      \n",
        "      \n",
        "      result.append(token.lemma_)\n",
        "      \n",
        "\n",
        "    \n",
        "    texto = ' '.join([str(elemento) for elemento in result if not elemento.isdigit()])\n",
        "    return texto\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kugnFPTT1GHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394a1a09-2d24-4031-b473-80da3bb4be1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-d6e9ee60877f>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['frases'][i] = textoProcessado\n",
            "<ipython-input-11-d6e9ee60877f>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['frases'][i] = textoProcessado\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                          cruzeiro ggaleixo zagueiro em é\n",
              "1                          adotar apartir agr agenda lotar\n",
              "2                                          amo fzr redação\n",
              "3        vontade sair existemal ficar casa ver série se...\n",
              "4                           dudacastrx mandar amg esquecer\n",
              "                               ...                        \n",
              "14002                                         foder porrar\n",
              "14003            vtxzn porra tá malucooo larica pprt kkkkk\n",
              "14004    thaidoback não sinto minhas pernas thaimeu joe...\n",
              "14005                                       se foder porra\n",
              "14006     vtxzn porra tá malucooo nem na larica pprt kkkkk\n",
              "Name: frases, Length: 14004, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "process = PLNprocess() \n",
        "for i, c in enumerate(df['frases']): \n",
        "  textoProcessado = process.preprocessamento(c) \n",
        "  df['frases'][i] = textoProcessado \n",
        "\n",
        "df['frases'] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQaAoz4z7yez"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(df['frases'])\n",
        "with open('vectorinizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C6V-vxMQcj4"
      },
      "source": [
        "# teste de algoritimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzlAe-QOJpl_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "x = df['frases']\n",
        "y = df['palavrão']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state = 0)\n",
        "\n",
        "x_train = vectorizer.transform(x_train)\n",
        "x_test = vectorizer.transform(x_test)\n",
        "\n",
        "\n",
        "\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test =x_test.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGJpdKO37Qg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "LGBM = LGBMClassifier(learning_rate =0.1, num_leaves = 100)\n",
        "svm = SVC(C=20, random_state=0)\n",
        "florest = RandomForestClassifier(200, random_state=0, criterion = 'entropy', max_features='log2')\n",
        "regression = LogisticRegression(penalty = 'l1',C=7.5,solver = 'liblinear',random_state=0) \n",
        "\n",
        "LGBM.fit(x_train, y_train)\n",
        "florest.fit(x_train, y_train)\n",
        "regression.fit(x_train, y_train)\n",
        "svm.fit(x_train, y_train)\n",
        "\n",
        "previsoes_f = florest.predict(x_test)\n",
        "previsoes_r = regression.predict(x_test)\n",
        "previsoes_s = svm.predict(x_test)\n",
        "previsoes_l = LGBM.predict(x_test)\n",
        "\n",
        "print(f\"floresta  {accuracy_score(y_test, previsoes_f)}, regressão logistica: {accuracy_score(y_test, previsoes_r)} svm: {accuracy_score(y_test, previsoes_s)} LGBM: {accuracy_score(y_test, previsoes_l)} \")\n",
        "\n",
        "print(classification_report(y_test, previsoes_r))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeldyJ3Df5YU"
      },
      "source": [
        "# Interferencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYyRGiyyf4xw"
      },
      "outputs": [],
      "source": [
        "frasesInferencia = {\n",
        "\n",
        "    'Eu vou a escola':0,\n",
        "    'De acordo com a constituição de 1988':0,\n",
        "    'eu te amo':0 ,\n",
        "    'hoje eu tava andando e caiu um pedasso de pau na minha cabeça':0,\n",
        "    'qual teclado compensa mais um redragon ou um razer':0,\n",
        "    'oi eai como você está':0,\n",
        "    'oi, você que ir no açai hoje':0,\n",
        "    'dessa forma podemos concluir que cada vez mais termos mudanças na educação':0,\n",
        "    'apesar de aparentem extremamente potentes, computadores quanticos não resolveram todos os nossos problemas':0,\n",
        "    'por que que para eles funcionaram precisam estar perto do 0 absoluto':0,\n",
        "    'para que não haja interferencia nas particulas':0,\n",
        "    'mas ainda sim não ta errado isso':0,\n",
        "    'eu acho que podemos fazer melhor':0, \n",
        "    'ele fazia bulling comigo':0,\n",
        "    'o manual desse rologio é muito bem feito':0,\n",
        "    'tava pensando em virar lider de turma so pelo presente':0,\n",
        "    'meu animal favorito é lagosta, e o seu':0,\n",
        "    'meu lapiz novo é muito bom':0,\n",
        "    'essa prova tava muito chata':0,\n",
        "    'meu computador começou a travar do nada':0,\n",
        "    'eu amo luz rgb':0,\n",
        "    'namora comigo':0,\n",
        "    'aque cara é muito chato':0,\n",
        "    'se eu fosse você não dava moral pra isso':0,\n",
        "    'não fica assim, vai dar certo':0,\n",
        "   \n",
        "    'Vai se foder':1,\n",
        "    'podemos concluir que os games estarão cada vez mais presentes na educação':0,\n",
        "    'eu adoro sorvete. ai hoje quando eu tava na frente da padaria eu fiquei puto com preço':1,\n",
        "    'Esse politicos vagabundo tem de serem presos':1,\n",
        "    'A puta que pariu que essa porra é assim':1,\n",
        "    'namoral vai tomar no cu':1,\n",
        "    'adiciona ele não, esse cara é muito fudido': 1,\n",
        "    'se fosse eu mandava metia o fodasse e ia pra proxima':1,\n",
        "    'se aquele viado não calar a boca eu vo ate ali dar um murro nele':1,\n",
        "    'era pra isso tar pronto hoje seu vagabundo': 1,\n",
        "    'para de foder com o trabalho':1,\n",
        "    'que merda, bati o dedinho na porta':1,\n",
        "    'caralho ta bem mais caro':1,\n",
        "    'ele disse: toma no cu':1,\n",
        "    'você não faz ideia de como eu odeio aquele viado':1,\n",
        "    'Esse cara e chato pra caralho':1,\n",
        "    'essa puta fica andando por ai com a calsa no cu':1,\n",
        "    'pora você fudeo com tudo':1,\n",
        "    'você ta fudido':1,\n",
        "    'compra logo essa porra':1,\n",
        "    'que buceta':1,\n",
        "    'para de andar com esses vagabundos, vai acabar com a sua reputação':1,\n",
        "    'se cabelo fosse importante não nascia no cu':1,\n",
        "    'seriao depois dessa você ta bem fudido':1,\n",
        "    'merda, esqueci o dever':1\n",
        "    }\n",
        "\n",
        "\n",
        "l = []\n",
        "pre = []\n",
        "for frase,label in frasesInferencia.items():\n",
        "    frase = process.preprocessamento(frase)\n",
        "    vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "    p = regression.predict(vector)\n",
        "    l.append([label])\n",
        "    pre.append(p)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(classification_report(l, pre))\n",
        "print(accuracy_score(l, pre))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mEmWm65yZqC"
      },
      "outputs": [],
      "source": [
        "#Frases que ele errou:\n",
        "for frase,label in frasesInferencia.items():\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  if label != predicao:\n",
        "    print(f'Na frase: \\n {frase} \\nVALOR REAL: {label}\\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnwX2b2Ky2W5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(l, pre, labels=regression.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                        display_labels=regression.classes_)\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nf9qae9-2mT"
      },
      "source": [
        "Matrix de confusão do df de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-yymwsG-2O4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test, previsoes_r, labels=regression.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                        display_labels=regression.classes_)\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGBAi_j51X1h"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_roc_curve(modelo, X_teste, y_teste):\n",
        "    # Obtém as probabilidades previstas e os valores verdadeiros\n",
        "    y_pred_proba = modelo.predict_proba(X_teste)[:, 1]\n",
        "    y_true = y_teste\n",
        "\n",
        "    # Calcula a curva ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plota a curva ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (área = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Taxa de Falso Positivo')\n",
        "    plt.ylabel('Taxa de Verdadeiro Positivo')\n",
        "    plt.title('Curva ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "plot_roc_curve(regression, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n0ILX09V24e"
      },
      "source": [
        "\n",
        "\n",
        "# Análise exploratória do dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC49BEA6V5MN"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x='palavrão',data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcyYK1xiev9Q"
      },
      "source": [
        "Quantidade ralativamente igual de palavras com e sem palavrões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQfGdKgLZdzR"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "stop = pt_core_news_sm.load().Defaults.stop_words\n",
        "stop.add('pra')\n",
        "stop.add('ão')\n",
        "wordcloud = WordCloud(stopwords=stop).generate(' '.join(df.frases.values.tolist()))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5A4bqLyqohh"
      },
      "source": [
        "# Teste em redações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSMBj51Nn23x"
      },
      "outputs": [],
      "source": [
        "a = 'Querido filho,\\n Fiquei sabendo que você terá seu primeiro debate fudido na próxima semana. Eu já conheço bem suas ideias antifeministas e sei que a feminista que debaterá com você é extremista. Sendo seu pai, sinto que é meu dever te lhe dar alguns conselhos.\\n Quando for debater, mantenha a calma e não ataque a argumentadora oponente, mas sim seus argumentos. Esse é o primeiro passo para que o debate seja civilizado e produtivo. Use a razão e a lógica. Caso a feminista faça o oposto disso, e, por exemplo, descarte os seus argumentos meramente por você ser homem, branco, heterossexual e cristão, explique ao público que ela está usando a famosa falácia ad hominem.\\n Evite e aponte, meu querido filho, todas as falácias lógicas. Falácias destroem o bom debate. Caso a feminista se esquive das críticas contra o feminismo, afirmando que aquilo que está sendo criticado por você chama-se \"femismo\" e não \"feminismo\", avise-a que ela está utilizando a falácia do escocês de verdade.\\n  \\n Mantenha-se firme também na defesa pela liberdade de expressão. Essa é outra regra de ouro para o bom debate. Não importa o quão verbalmente agressiva seja a feminista ou quão repulsivas pareçam ser suas ideias, lembre-se que ela possui o direito de falar tudo aquilo que realmente pensa. Se tudo o que ela fala for de fato tão horrível e irracional assim, então será fácil para você desconstruir publicamente os argumentos dela. Lembre-se daquilo que sempre te ensinei: tentar calar o debatedor adversário à força, como se estivesse cometendo um crime por expor o que pensa, é para aqueles que não conseguem convencer os demais através do uso da lógica e da razão, então precisam fazer isso através de meios coercitivos.\\n Portanto, filho, se alguém nesse debate utilizar constantemente argumentos falaciosos ou tentar calar à força o adversário, alegando estar combatendo um \"discurso de ódio\", então que esse alguém não seja você. Não podemos controlar a feminista com quem você debaterá, mas você tem controle sobre si próprio. Faça a sua parte e tenha um bom debate!\\n Com carinho,\\n Seu amado Pai.'\n",
        "a = a.replace('\\n', '')\n",
        "a = a.split('.')\n",
        "a = [i.strip() for s in a for i in s.split(',')]\n",
        "\n",
        "for frase in a:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy4K1MQspPXC"
      },
      "outputs": [],
      "source": [
        "b = 'O conhecimento é fundamental para que haja uma opinião crítica sobre um determinado assunto, esse é o ponto de partida de um argumento num debate. Sobretudo há um grande problema com grande parte da população brasileira, que é a falta de leitura.\\n O Brasil é um país com uma baixa taxa de leitores, de acordo com pesquisas, muito dos brasileiros tem deixado os livros em segundo plano, isso porque o uso de celulares, tabletes e notebooks são os maiores concorrentes com o popular e conhecido livro.\\n A leitura proporciona, além do conhecimento, novos horizontes para que as pessoas possam ter opiniões formadas e entender sobre diversos assuntos. Assim num debate, ou numa discussão, não haverá aquele velho discurso que todos usam e que ninguém nem sabe o que realmente significa, mas que compartilham em redes sociais como se fossem os próprios autores.\\n Um debate que gera resultados satisfatórios é feito por pessoas com habilidades adquiridas através da leitura, da busca pelo conhecimento e pela criticidade, debater não significa brigar, é modo de cada um mostrar com argumentos válidos seu posicionamento, é uma conversa que pode gerar controvérsias, mas que é aceita e no final agrega sabedoria.'\n",
        "b = b.replace('\\n', '')\n",
        "b = b.split('.')\n",
        "b = [i.strip() for s in b for i in s.split(',')]\n",
        "\n",
        "for frase in b:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df-eELy3pgid"
      },
      "outputs": [],
      "source": [
        "c = 'No âmbito escolar, a discriminação vem gerando cada vez mais atos de violências devido a diversidade de opções sexuais. A partir destes aspectos os preconceitos se originam, mas o respeito é direito e dever de toda a população.\\n O preconceito dos alunos com os outros que optam por uma escolha sexual diferente é perceptível, como consequência seguinte a violência não sendo ela apenas física, também sentimental e etc. É frequente a decorrência de fatos de discriminação por esse motivo, como exemplo podemos citar inúmeros casos, que já foram retratados nas telenovelas da Rede Globo onde pessoas homossexuais foram vítimas de preconceito e discriminação.\\n Segundo a Constituição Brasileira, todos são iguais perante a lei e todos os brasileiros e estrangeiros residentes tem direito à vida, à liberdade, à igualdade, à segurança e à propriedade sem separação de qualquer natureza.\\n Portanto, sendo um problema á sociedade, as rejeições estão presentes no nosso cotidiano escolar. Necessitando assim, de uma criação de programas sobre o direito e dever dos alunos de respeitar as diferenças sexuais de seus colegas, contando com o apoio da secretaria municipal de educação para desenvolver palestras educativas, os pais pais também precisam estar colaborando para que os resultados sejam efetivos gerando assim uma convivência pacifica dentro e fora do ambiente escolar.'\n",
        "c = c.replace('\\n', '')\n",
        "c = c.split('.') \n",
        "c= [i.strip() for s in c for i in s.split(',')]\n",
        "\n",
        "for frase in c:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkQ7U7tbp1Pq"
      },
      "outputs": [],
      "source": [
        "d = \"\"\"O aquecimento global vem afetando o mundo nos últimos anos, ess fator e algo natural porem está aumentando drasticamente pela quantidade de combustíveis quem são queimados diariamente, assim afetando o ambiente onde vivemos, ocasionando uma elevação na temperatura ambiente.\\n A partir disso surgem vários problemas que afetam o habitat natural de vários animais, e um deles e o derretimento das geleiras que obrigam os animais como o urso polar a mudar seus hábitos de caça.\\n Muitos ursos para se alimentarem começaram a invadir cidades, a relatos de que os ursos atacam pessoas para se alimentar. Muitos estão pensando na hipótese de caça aos ursos porem esse método e proibido em lei, além disso, a agência do Meio Ambiente russa negou a licença para abater essa espécie pois a mesma se encontra em extinção.\\n Portanto, medidas são necessárias para resolver esse impasse podendo ter algum órgão governamental responsável pelo meio ambiente para se responsabilizar de resgatar e alimentar esses animais para evitar futuros ataques contra pessoas. A Rússia o quinto pais do ranking de emissores de queima de combustíveis para que ela possa reduzir a queima de combustíveis deve investir em um combustível mais \"limpo\" para o ambiente por exemplo o etanol além de estimular o uso do mesmo\"\"\"\n",
        "d = d.replace('\\n', '')\n",
        "d = d.split('.')\n",
        "d = [i.strip() for s in d for i in s.split(',')]\n",
        "\n",
        "for frase in d:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF-0IGI0qNb9"
      },
      "outputs": [],
      "source": [
        "e = \"\"\"'Em dezembro de 2019, o mundo foi acometido por um vírus de alarmante potencial pandêmico (oriundo de Wuhan, na China), que rapidamente espalhou-se por todo o globo terrestre. Gerando impactos sociais, como a restrição da circulação Humana, e a pausa dos trabalhos não essências para a sobrevivência. O que também levou á impactos ambientais, porém pouco duradouros.\\n \\n \\n \\n Devido a paralisação de atividades econômicas, a qualidade do ar teve uma melhora notória, o que acarreta diversos benefícios ao meio ambiente e consequentemente a nossa Saúde. Doenças respiratórias como: Renite, Asma, pneumonia, câncer de pulmão e etc. São em parte, consequências do estado deplorável do ar. No entanto, tais mudanças não são permanentes, tendo em consideração que as mudanças de hábitos se devem as restrições sociais impostas pela Organização Mundial de Saúde (OMS), e que os esforços para a recuperação do estado econômico do país pós pandemia, serão Extremamente prejudiciais, considerando que a produção de poluentes como o CO2 serão exacerbadas.\\n \\n \\n \\n No final do ano de 2019, a Organização das Nações Unidas (ONU) afirmou que para conter catástrofes naturais, seria necessário uma diminuição de 7,6% de emissão de carbono por 10 anos. O que de fato é impossível considerando-se que o Brasil, assim como tantos outros países, está em um processo de desenvolvimento que demanda uma alta atividade industrial. Mesmo com os esforços feitos em meio a pandemia, houve a diminuição de apenas 5% de carbono emitido no ar.\\n \\n Por essas considerações, é nítido que os impactos ambientais em meio a Pandemia, nos traz falsas esperanças de melhoras.\"\"\"\n",
        "e= e.replace('\\n', '')\n",
        "e= e.split('.')\n",
        "e = [i.strip() for s in e for i in s.split(',')]\n",
        "\n",
        "for frase in e:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbiqVpS-qZoA"
      },
      "outputs": [],
      "source": [
        "f  = \"\"\"'Após as Revoluções Industriais o homem passou exigir mais do meio-ambiente utilizando seus recursos e poluindo-o em níveis alarmantes, tudo isso para manter a demanda de consumo que com o passar dos anos foi aumentando. Inesperadamente surge uma doença, a Covid-19, que posteriormente é classificada como pandemia. As indústrias do mundo todo tiveram que interromper suas atividades, o que acabou impactando positivamente o planeta na questão ambiental. \\n \\n As pandemias sempre foram presentes no decorrer da história, mas nenhuma delas até então havia ocorrido em um mundo tão globalizado como o que vivemos, com isso o poder de disseminação da doença é avassalador, o que consequentemente torna seus impactos maiores e mais preocupantes. Isso obrigou a população mundial a mudar os hábitos e trouxe mudanças inimagináveis. \\n \\n No Brasil, grandes cidades como São Paulo se viram forçadas a traçar planos a fim de conter o avanço do vírus, utilizando- se de medidas que antes jamais haviam sido cogitadas, mas que foram necessárias. Com isso o tempo que as pessoas passam em casa aumentou e o uso de automóveis diminuiu, fazendo com que a emissão de seus gases se reduzissem, dando um “alívio” à atmosfera. \\n \\n  Em virtude dos fatos apresentados, somos levados a acreditar que o padrão de vida atual é uma enorme ameaça à saúde do meio- ambiente. Sendo assim, é fundamental que órgão federais como o Ministério do Meio-Ambiente adotem posturas que incentivem a fiscalização dessas indústrias e puna quem desrespeitar as normas (respeitando os Diretos Humanos). Junto disso, se faz necessário que atitudes também sejam tomadas pelo Ministério da educação (MEC), para que as pessoas tenham o senso ecológico e a responsabilidade socioambiental aflorados desde a pré-escola. A esperança de um povo mais responsável e um planeta mais saudável, se torna mais real com cada uma fazendo sua parte.'\"\"\"\n",
        "\n",
        "f= f.replace('\\n', '')\n",
        "f= f.split('.')\n",
        "f = [i.strip() for s in f for i in s.split(',')]\n",
        "\n",
        "for frase in f:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g  = \"\"\"'O conhecimento é fundamental para que haja uma opinião crítica sobre um determinado assunto, caralho, esse é o ponto de partida de um argumento num debate. Sobretudo há um grande problema com grande parte da população brasileira, que é a falta de leitura. \\n O Brasil é um país com uma baixa taxa de leitores, de acordo com pesquisas, todos os brasileiros tem deixado os livros em segundo plano, isso porque o uso de celulares, tabletes e notebooks são os maiores concorrentes com o popular e conhecido livro. \\n  A leitura proporciona, além do conhecimento, novos horizontes para que as pessoas possam ter opiniões formadas e entender sobre diversos assuntos. Assim num debate, ou numa discussão, não haverá aquele velho discurso que todos usam e que ninguém nem sabe o que realmente significa, mas que compartilham em redes sociais como se fossem os próprios autores. \\n  Um debate que gera resultados satisfatórios é feito por pessoas com habilidades adquiridas através da leitura, da busca pelo conhecimento e pela criticidade, debater não significa brigar, é modo de cada um mostrar com argumentos válidos seu posicionamento, é uma conversa que pode gerar controvérsias, mas que é aceita e no final agrega sabedoria.'\"\"\"\n",
        "\n",
        "\n",
        "g= g.replace('\\n', '')\n",
        "g= g.split('.')\n",
        "g = [i.strip() for s in g for i in s.split(',')]\n",
        "\n",
        "for frase in g:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n"
      ],
      "metadata": {
        "id": "TvGJekAnlknn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h  = \"\"\"Umas das causas que mais têm se visto é machismo que leva à um morte de uma mulher , pode ser término de relacionamento ou não, ciúmes, possesividade, achar que dona dela que essa não tem vida so que eles querem um outro ponto a ser levado em consideração é quanto já foi sofrido antes do crime acontecer, será ja tentaram ajuda para se livrar ,leis que podiam ajudar mas nem sempre tem sua eficaz quando precisa-se como a Lei Maria da Penha, tem como ajudar para manter longe esse agressor nem sempre é suficiente é que vemos muito no jornal hoje mais caso de feminicidio,amanha e depois. \\n Uma campanha so do governo também não é suficiente para alertar mulheres que vêm pasando por violencia pois para que essas não sejam mortas como outras tantas tem vir acompanhada de tratamento psicologicos acompanhado de assistência para saibam que não estão sozinha ée todas possam ser ajudadas antes quemais mortes aconteçam.'\"\"\"\n",
        "\n",
        "\n",
        "h= h.replace('\\n', '')\n",
        "h= h.split('.')\n",
        "\n",
        "\n",
        "for frase in h:\n",
        "  vector = vectorizer.transform([process.preprocessamento(frase)])\n",
        "  predicao = regression.predict(vector)\n",
        "  proba = regression.predict_proba(vector)\n",
        "  \n",
        "  print(f'Na frase: \\n {frase} \\nVALOR PREDITO: {predicao} com {round(max(proba[0])*100,2)}% de probabilidade\\n\\n')\n"
      ],
      "metadata": {
        "id": "pS4cArgIm8X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f4L0S4kkWJk"
      },
      "source": [
        "# Explicação tecnica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyh5naiDkn87"
      },
      "source": [
        "**Quantidade de dados** foi de 14 mil 10 sem palavrão e 1 sem palavrão pois dessa forma criamos uma tendencia no modelo a prever 0 o que evita falsos positivos além de que no mundo real a maior parte das redações não contem palavrões.\n",
        "Os valores 10k e 4k foram escolidos com base em testes. Mais que isso faz uma diferença muito pequena o que não justifica o esforço computacional e uma quantidade menor do que 4k para 1 não é suficiente para o modelo aprender todos os palavrões.\n",
        "\n",
        "\n",
        "\n",
        "**Preprocessamento:** Fiz a remoção de puntuação e stop words por não serem relevantes a classe ( cheguei essa conclusão ultilizando corelação). Ultilizei Tf-idf pois nesse caso palavrões não aparecem muitas vezes e por isso recebem peso maior.\n",
        "\n",
        "**Inferencia** Os dados não estão presentes no dataset mais ainda sim o modelo acerta tudo. Podemos concluir que é um modelo praticamente perfeito"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evUM1oBP6uXD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(regression, f)\n",
        "with open('vectorinizer.pkl', 'wb') as f:\n",
        "    vectorinizer.dump(regression, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_string(self,text):\n",
        "        \"\"\"\n",
        "        Verifica se uma string é válida, ou seja, não é composta somente por espaços em branco,\n",
        "        pontuações ou numerais.\n",
        "\n",
        "        :param string: A string a ser verificada.\n",
        "        :type string: str\n",
        "        :return: Retorna True se a string é válida e False caso contrário.\n",
        "        :rtype: bool\n",
        "        \"\"\"\n",
        "        if text.strip() and not all(char.isdigit() or char.isspace() or char in string.punctuation for char in text):\n",
        "            return True\n",
        "        return False\n",
        "class TokenCRIA():\n",
        "    \"\"\"\n",
        "    Classe para padronizar o acesso a informações de tokens no formato CRIA\n",
        "\n",
        "    Atributos:\n",
        "    ----------\n",
        "        texto : str\n",
        "            Texto para compor Token\n",
        "        span : tuple\n",
        "            Tupla contendo posição inicial e final do texto na string original\n",
        "    \"\"\"\n",
        "    def init(self,palavra,span):\n",
        "        self.text = str(palavra)\n",
        "        self.start = int(span[0])\n",
        "        self.end = int(span[1])\n",
        "        return\n",
        "\n",
        "    def repr(self):\n",
        "        return self.text\n",
        "\n",
        "def divideSentencas(texto):\n",
        "        \"\"\"\n",
        "        Divide as sentenças de um texto.\n",
        "        ...\n",
        "\n",
        "        Atributos:\n",
        "        ---------\n",
        "            texto : str\n",
        "                String a ser dividida\n",
        "\n",
        "        Retorna:\n",
        "        ---------\n",
        "                list[TokenCRIA] : list\n",
        "                    Lista de TokenCRIA com sentenças do texto.\n",
        "        \"\"\"\n",
        "        sentence_separator = '.?!'\n",
        "        lista_expressoes_abreviadas = [\n",
        "            r'\\d',r'Art',r'Lei',r'N',r'Dr',r'Dra',\n",
        "            r'Sr',r'Sra',r'Av',r'Sec',r'Apt']\n",
        "        negative_lookbehind = '?<!'\n",
        "        regular_exceptions = ''.join([f'({negative_lookbehind}{expressao})' for expressao in lista_expressoes_abreviadas])\n",
        "        sentence_endings_pattern = f\"{regular_exceptions}([{sentence_separator}])\"\n",
        "        splitStep = re.split(sentence_endings_pattern, texto)\n",
        "        splitedList = [splitStep[index]+splitStep[index+1] for index in range(0,len(splitStep),2) if index!= len(splitStep)-1]\n",
        "        sentences = [sentenca for sentenca in splitedList if is_valid_string(sentenca)]\n",
        "        pos_finais = [texto.index(sentenca) + len(sentenca) for sentenca in sentences]\n",
        "        pos_iniciais = [texto.index(sentenca) for sentenca in sentences]\n",
        "\n",
        "divideSentencas('Querido filho,\\n Fiquei sabendo que você terá seu primeiro debate fudido na próxima semana. Eu já conheço bem suas ideias antifeministas e sei que a feminista que debaterá com você é extremista. Sendo seu pai, sinto que é meu dever te lhe dar alguns conselhos.\\n Quando for debater, mantenha a calma e não ataque a argumentadora oponente, mas sim seus argumentos. Esse é o primeiro passo para que o debate seja civilizado e produtivo. Use a razão e a lógica. Caso a feminista faça o oposto disso, e, por exemplo, descarte os seus argumentos meramente por você ser homem, branco, heterossexual e cristão, explique ao público que ela está usando a famosa falácia ad hominem.\\n Evite e aponte, meu querido filho, todas as falácias lógicas. Falácias destroem o bom debate. Caso a feminista se esquive das críticas contra o feminismo, afirmando que aquilo que está sendo criticado por você chama-se \"femismo\" e não \"feminismo\", avise-a que ela está utilizando a falácia do escocês de verdade.\\n  \\n Mantenha-se firme também na defesa pela liberdade de expressão. Essa é outra regra de ouro para o bom debate. Não importa o quão verbalmente agressiva seja a feminista ou quão repulsivas pareçam ser suas ideias, lembre-se que ela possui o direito de falar tudo aquilo que realmente pensa. Se tudo o que ela fala for de fato tão horrível e irracional assim, então será fácil para você desconstruir publicamente os argumentos dela. Lembre-se daquilo que sempre te ensinei: tentar calar o debatedor adversário à força, como se estivesse cometendo um crime por expor o que pensa, é para aqueles que não conseguem convencer os demais através do uso da lógica e da razão, então precisam fazer isso através de meios coercitivos.\\n Portanto, filho, se alguém nesse debate utilizar constantemente argumentos falaciosos ou tentar calar à força o adversário, alegando estar combatendo um \"discurso de ódio\", então que esse alguém não seja você. Não podemos controlar a feminista com quem você debaterá, mas você tem controle sobre si próprio. Faça a sua parte e tenha um bom debate !\\n Com carinho,\\n Seu amado Pai.')"
      ],
      "metadata": {
        "id": "Fs5Q1E-hx63x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wCkXQuxKFYkS",
        "H11wiPj6HLMO",
        "IeldyJ3Df5YU",
        "-n0ILX09V24e"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}